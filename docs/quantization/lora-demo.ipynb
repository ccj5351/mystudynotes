{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31d16cea-c267-4e2f-86fd-232293f7cae5",
   "metadata": {},
   "source": [
    "# LoRA from Scratch \n",
    "\n",
    "> see the original blog: https://lightning.ai/lightning-ai/studios/code-lora-from-scratch?section=featured&tab=overview "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eff363c-8498-4b62-ba51-7db0487d11c5",
   "metadata": {},
   "source": [
    "## 1. LoRA From Scratch – Implement Low-Rank Adaptation for LLMs in PyTorch\n",
    "LoRA, which stands for [Low-Rank Adaptation](https://arxiv.org/abs/2106.09685), is a popular technique to finetune LLMs more efficiently. Instead of adjusting all the parameters of a deep neural network, LoRA focuses on updating only a small set of low-rank matrices.\n",
    "\n",
    "This Studio explains how LoRA works by coding it from scratch, which is an excellent exercise for looking under the hood of an algorithm.\n",
    "\n",
    "\n",
    "## 2. Understanding LoRA\n",
    "Pretrained LLMs are often dubbed foundation models because of their versatility across various tasks. However, it is often useful to adapt a pretrained LLM for a specific dataset or task, which we can accomplish via finetuning.\n",
    "\n",
    "Finetuning allows the model to adapt to specific domains without costly pretraining, yet updating all layers can still be computationally expensive, especially for larger models.\n",
    "\n",
    "LoRA offers a more efficient alternative to regular finetuning. As discussed in more detail in the paper [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685), LoRA approximates a layer's weight changes during training, _**ΔW**,_ in a low-rank format.\n",
    "\n",
    "For instance, whereas in regular finetuning, we compute the weight updates of a weight matrix _**W**_ as **_ΔW_**, in LoRA, we approximate **_ΔW_** through the matrix multiplication of two smaller matrices AB, as illustrated in the figure below. (If you are familiar with PCA or SVD, consider this as decomposing **_ΔW_** into **_A_** and **_B_**.)\n",
    "\n",
    "For instance, whereas in regular finetuning, we compute the weight updates of a weight matrix _**W**_ as **_ΔW_**, in LoRA, we approximate **_ΔW_** through the matrix multiplication of two smaller matrices AB, as illustrated in the figure below. (If you are familiar with PCA or SVD, consider this as decomposing **_ΔW_** into **_A_** and **_B_**.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5630a308-d4a2-4753-81bf-d0451614f6c6",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "<img alt = \"A comparison between the weight updates during the forward pass in regular finetuning (left) and LoRA (right).\" src=\"../files/lora_01.png\" width=\"900\"/>\n",
    "<em>A comparison between the weight updates during the forward pass in regular finetuning (left) and LoRA (right).</em></br></br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b703dc6-5b67-4ecf-8288-d9b02e82b0b1",
   "metadata": {},
   "source": [
    "Note that **_r_**, in the figure above, is a hyperparameter here that we can use to specify the rank of the low-rank matrices used for adaptation. A smaller **_r_** leads to a simpler low-rank matrix, which results in fewer parameters to learn during adaptation. This can lead to faster training and potentially reduced computational requirements. However, with a smaller **_r_**, the capacity of the low-rank matrix to capture task-specific information decreases.\n",
    "\n",
    "To provide a concrete example, suppose the weight matrix W of a given layer has a size of 5,000x10,000 (50M parameters in total). If we choose a rank _**r=8**_, we initialize two matrices: the 5,000x8-dimensional matrix B and the 8x10,000-dimensional matrix A. Added together, A and B have only 80,000 + 40,000 = 120,000 parameters, which is 400 times smaller than the 50M parameters in regular finetuning via **_ΔW_**.\n",
    "\n",
    "In practice, it’s important to experiment with different **_r_** values to find the right balance to achieve the desired performance on the new task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da549bca-a64b-4d38-8d1f-1bb96bf3e6f8",
   "metadata": {},
   "source": [
    "## 3. Coding LoRA from Scratch\n",
    "\n",
    "Since conceptual explanations can sometimes be abstract, let's now implement LoRA ourselves to get a better idea of how it works. In code, we can implement a LoRA layer as follows:\n",
    "Here we implement LoRA as the following equation. A little abuse use of $A$ and $B$, opposite to the above figure $A$ and $B$.\n",
    "\n",
    "$$ \\begin{align*} Y &= X(W+\\Delta W) = XW + XAB  \\\\\n",
    "&= X_{b\\times c_{\\text{in}}} W_{c_{\\text{in}} \\times c_{\\text{out}}} + X_{b\\times c_{\\text{in}}} A_{c_{\\text{in}} \\times r} B_{r \\times c_{\\text{out}}} \n",
    "\\end{align*} \\quad \\text{with } r \\ll min(c_{\\text{in}}, c_{\\text{out}})\n",
    "$$\n",
    "\n",
    "where, learnable parameters $A$ and $B$ are initialize as $A \\sim \\mathcal{N}(0,\\, \\sigma^{2})$ and $B=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44e402c1-b5cb-4771-a03c-3e5a1f8bc2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class LoRALayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        self.W_a = torch.nn.Parameter(torch.randn(in_dim, rank) * std_dev)\n",
    "        self.W_b = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.alpha * (x @ self.W_a @ self.W_b)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7b4fdab-8b71-4066-b08d-eefa0631eaa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5745]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# a simple linear layer with 10 inputs and 1 output\n",
    "# requires_grad=False makes it non-trainable\n",
    "linear_layer = torch.nn.Linear(\n",
    "    in_features = 10, out_features = 1, bias=True)\n",
    "\n",
    "# a simple example input\n",
    "x = torch.rand((1, 10))\n",
    "\n",
    "linear_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57b3afb-544a-417d-9502-219955e43ae0",
   "metadata": {},
   "source": [
    "In the code above, `in_dim` is the input dimension of the layer we want to modify using LoRA, and `out_dim` is the respective output dimension of that layer.\n",
    "\n",
    "We previously discussed that the rank of the matrices **_A_** and **_B_** (Note: here we use `self.W_a` and `self.W_b` for the matrices **_A_** and **_B_**, respectively) is a hyperparameter that controls the complexity and the number of additional parameters introduced by LoRA.\n",
    "\n",
    "However, looking at the code above, we added another hyperparameter, the scaling factor `alpha`. This factor determines the magnitude of the changes introduced by the LoRA layer to the model's existing weights: `alpha * (x @ A @ B)`. A higher value of `alpha` means larger adjustments to the model's behavior, while a lower value results in more subtle changes.\n",
    "\n",
    "Another thing to note is that we initialized `A` with small values from a random distribution. Here, the standard deviation of this distribution is determined by the square root of the rank (this choice ensures that the initial values in `A` are not too large.) However, we initialized `B` with zeros. The rationale here is that at the beginning of the training, before `A` and `B` are updated via backpropagation, the `LoRALayer` does not impact the original weights because _**AB=0**_ if **_B=0_**.\n",
    "\n",
    "Note that LoRA is usually applied to a neural network's linear (feedforward) layers. For example, suppose we have a simple PyTorch model or module with two linear layers (e.g., this could be the feedforward module of a transformer block). And suppose this module's forward method looks like as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "874f6d92-bf2f-4a86-ba0b-e388c292645c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearWithLoRA(torch.nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7500b81b-10ce-4f08-b4bd-8e405f864058",
   "metadata": {},
   "source": [
    "Replace linear layer with LoRA layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2016214d-3d0b-46b4-b47d-63e44b2753ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5745]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_layer = LinearWithLoRA(linear=linear_layer, rank=8, alpha=1)\n",
    "lora_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1995aa9b-5071-448f-8dd0-49d14b79893a",
   "metadata": {},
   "source": [
    "Note that the LoRA layer will not change the linear layer output until it's trained because its `W_b` weight matrix is initialized to all zeros. For LoRA to take effect, we have to train the model. Below is a simple toy example.\n",
    "\n",
    "Let's simulate a simple weight update:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7db16db-f687-4b2c-b2f4-176b9cf34e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_layer.lora.W_b = torch.nn.Parameter(lora_layer.lora.W_b + 0.01 * x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f699d9-d0cf-4466-b3d3-26d7319d2a01",
   "metadata": {},
   "source": [
    "We can now see that the output has changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58fe2b79-955a-45a6-8826-a6285ec1be83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5863, -0.5758, -0.5779, -0.5800, -0.5814, -0.5766, -0.5887, -0.5811,\n",
       "         -0.5859, -0.5892]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01403adc-fca1-400c-b2c7-699b351b9839",
   "metadata": {},
   "source": [
    "## 4. Finetuning with LoRA -- A Hands-On Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53006977-ee9d-4e34-aa9f-9a4b83b25198",
   "metadata": {},
   "source": [
    "### 1) Loading the dataset into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdd7b8f4-a21f-405e-8cae-22961a989546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from local_dataset_utilities import download_dataset, load_dataset_into_to_dataframe, partition_dataset\n",
    "from local_dataset_utilities import IMDBDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7714ade5-700c-4ca2-b253-afddffcb54cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    print(\"Please switch to a GPU machine before running this notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eabdec43-75eb-42d1-99c4-cb07d42b7c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = (\"test.csv\", \"train.csv\", \"val.csv\")\n",
    "download = True\n",
    "\n",
    "for f in files:\n",
    "    if not os.path.exists(os.path.join(\"data\", f)):\n",
    "        download = False\n",
    "\n",
    "if download is False:\n",
    "    download_dataset()\n",
    "    df = load_dataset_into_to_dataframe()\n",
    "    partition_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d85448b-9df8-46c7-8282-c07f1de972a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(os.path.join(\"data\", \"train.csv\"))\n",
    "df_val = pd.read_csv(os.path.join(\"data\", \"val.csv\"))\n",
    "df_test = pd.read_csv(os.path.join(\"data\", \"test.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb89241-e726-4b0c-af47-a74f3ca6955f",
   "metadata": {},
   "source": [
    "### 2) Tokenization and Numericalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c476c8-1d59-4e2e-aaaa-c1abb984263c",
   "metadata": {},
   "source": [
    "**Load the dataset via `load_dataset`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da951e0f-052d-4ea3-8c00-dba36fede402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['index', 'text', 'label'],\n",
      "        num_rows: 35000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['index', 'text', 'label'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['index', 'text', 'label'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "imdb_dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"train\": os.path.join(\"data\", \"train.csv\"),\n",
    "        \"validation\": os.path.join(\"data\", \"val.csv\"),\n",
    "        \"test\": os.path.join(\"data\", \"test.csv\"),\n",
    "    },\n",
    ")\n",
    "\n",
    "print(imdb_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df54cc75-212d-4be7-ab69-95a7595d2768",
   "metadata": {},
   "source": [
    "**Tokenize the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08b40c64-da49-4320-9e70-db239f572ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer input max length: 512\n",
      "Tokenizer vocabulary size: 30522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/changjiang/miniconda3/envs/py310_pt220/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "print(\"Tokenizer input max length:\", tokenizer.model_max_length)\n",
    "print(\"Tokenizer vocabulary size:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72463108-9ac7-4a0e-8648-f1fc155e7115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ed65815-b75b-43ac-8e69-db537042eb4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c65b87dd30df4461a49f65321c7e5e38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/35000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73cf20b22f734a9fbcf82908ba97e745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d232eb074a584d8d86a34959591ac809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imdb_tokenized = imdb_dataset.map(tokenize_text, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e01793f3-c090-4f3a-82ad-8a653416d4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "del imdb_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "679cea54-7be7-4364-b7fa-4d0cb12e8191",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d7008af-9569-445d-9228-701c88bb9a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058a44d2-883e-45b1-a6ac-173f28548305",
   "metadata": {},
   "source": [
    "### 3) Set Up DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e860f2a-fb11-4fcb-ad0b-cdbc4b04ffaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, dataset_dict, partition_key=\"train\"):\n",
    "        self.partition = dataset_dict[partition_key]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.partition[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.partition.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9bfa8edb-c5c2-428e-8131-0e89011abd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IMDBDataset(imdb_tokenized, partition_key=\"train\")\n",
    "val_dataset = IMDBDataset(imdb_tokenized, partition_key=\"validation\")\n",
    "test_dataset = IMDBDataset(imdb_tokenized, partition_key=\"test\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=12,\n",
    "    shuffle=True, \n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=12,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=12,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eeaa5a3-80f6-46bf-9b2d-55cacd468c75",
   "metadata": {},
   "source": [
    "### 4) Initializing DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34c2858d-7377-47bb-834f-9173729c2d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d22a3d-436b-492b-a277-261715b30758",
   "metadata": {},
   "source": [
    "**Freeze all layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35408c4c-b0b6-42e3-b2cf-a66f2bb69f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5842ee6-7b5f-4b2c-9519-5248a6ec3e18",
   "metadata": {},
   "source": [
    "**Add LoRA layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5aa219c-edbc-47d5-aced-7c32fc7c2414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input model \n",
      "**************\n",
      " DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print (\"Input model \\n**************\\n\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69606014-56ee-43b1-81eb-5e99d9b82efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "lora_r = 8\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.05\n",
    "lora_query = True\n",
    "lora_key = False\n",
    "lora_value = True\n",
    "lora_projection = False\n",
    "lora_mlp = False\n",
    "lora_head = False\n",
    "\n",
    "layers = []\n",
    "\n",
    "assign_lora = partial(LinearWithLoRA, rank=lora_r, alpha=lora_alpha)\n",
    "\n",
    "for layer in model.distilbert.transformer.layer:\n",
    "    if lora_query:\n",
    "        layer.attention.q_lin = assign_lora(layer.attention.q_lin)\n",
    "    if lora_key:\n",
    "        layer.attention.k_lin = assign_lora(layer.attention.k_lin)\n",
    "    if lora_value:\n",
    "        layer.attention.v_lin = assign_lora(layer.attention.v_lin)\n",
    "    if lora_projection:\n",
    "        layer.attention.out_lin = assign_lora(layer.attention.out_lin)\n",
    "    if lora_mlp:\n",
    "        layer.ffn.lin1 = assign_lora(layer.ffn.lin1)\n",
    "        layer.ffn.lin2 = assign_lora(layer.ffn.lin2)\n",
    "if lora_head:\n",
    "    model.pre_classifier = assign_lora(model.pre_classifier)\n",
    "    model.classifier = assign_lora(model.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "179ba6e8-a352-43d2-a5f8-6a16b7950297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now model + LoRA \n",
      "**************\n",
      " DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): LinearWithLoRA(\n",
      "              (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): LinearWithLoRA(\n",
      "              (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print (\"Now model + LoRA \\n**************\\n\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42161291-2335-4846-8263-96562a8b6782",
   "metadata": {},
   "source": [
    "**Let us check if linear layers are frozen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "693bdf82-cca9-426f-aa65-6292698bcbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert.embeddings.word_embeddings.weight: False\n",
      "distilbert.embeddings.position_embeddings.weight: False\n",
      "distilbert.embeddings.LayerNorm.weight: False\n",
      "distilbert.embeddings.LayerNorm.bias: False\n",
      "distilbert.transformer.layer.0.attention.q_lin.linear.weight: False\n",
      "distilbert.transformer.layer.0.attention.q_lin.linear.bias: False\n",
      "distilbert.transformer.layer.0.attention.q_lin.lora.W_a: True\n",
      "distilbert.transformer.layer.0.attention.q_lin.lora.W_b: True\n",
      "distilbert.transformer.layer.0.attention.k_lin.weight: False\n",
      "distilbert.transformer.layer.0.attention.k_lin.bias: False\n",
      "distilbert.transformer.layer.0.attention.v_lin.linear.weight: False\n",
      "distilbert.transformer.layer.0.attention.v_lin.linear.bias: False\n",
      "distilbert.transformer.layer.0.attention.v_lin.lora.W_a: True\n",
      "distilbert.transformer.layer.0.attention.v_lin.lora.W_b: True\n",
      "distilbert.transformer.layer.0.attention.out_lin.weight: False\n",
      "distilbert.transformer.layer.0.attention.out_lin.bias: False\n",
      "distilbert.transformer.layer.0.sa_layer_norm.weight: False\n",
      "distilbert.transformer.layer.0.sa_layer_norm.bias: False\n",
      "distilbert.transformer.layer.0.ffn.lin1.weight: False\n",
      "distilbert.transformer.layer.0.ffn.lin1.bias: False\n",
      "distilbert.transformer.layer.0.ffn.lin2.weight: False\n",
      "distilbert.transformer.layer.0.ffn.lin2.bias: False\n",
      "distilbert.transformer.layer.0.output_layer_norm.weight: False\n",
      "distilbert.transformer.layer.0.output_layer_norm.bias: False\n",
      "distilbert.transformer.layer.1.attention.q_lin.linear.weight: False\n",
      "distilbert.transformer.layer.1.attention.q_lin.linear.bias: False\n",
      "distilbert.transformer.layer.1.attention.q_lin.lora.W_a: True\n",
      "distilbert.transformer.layer.1.attention.q_lin.lora.W_b: True\n",
      "distilbert.transformer.layer.1.attention.k_lin.weight: False\n",
      "distilbert.transformer.layer.1.attention.k_lin.bias: False\n",
      "distilbert.transformer.layer.1.attention.v_lin.linear.weight: False\n",
      "distilbert.transformer.layer.1.attention.v_lin.linear.bias: False\n",
      "distilbert.transformer.layer.1.attention.v_lin.lora.W_a: True\n",
      "distilbert.transformer.layer.1.attention.v_lin.lora.W_b: True\n",
      "distilbert.transformer.layer.1.attention.out_lin.weight: False\n",
      "distilbert.transformer.layer.1.attention.out_lin.bias: False\n",
      "distilbert.transformer.layer.1.sa_layer_norm.weight: False\n",
      "distilbert.transformer.layer.1.sa_layer_norm.bias: False\n",
      "distilbert.transformer.layer.1.ffn.lin1.weight: False\n",
      "distilbert.transformer.layer.1.ffn.lin1.bias: False\n",
      "distilbert.transformer.layer.1.ffn.lin2.weight: False\n",
      "distilbert.transformer.layer.1.ffn.lin2.bias: False\n",
      "distilbert.transformer.layer.1.output_layer_norm.weight: False\n",
      "distilbert.transformer.layer.1.output_layer_norm.bias: False\n",
      "distilbert.transformer.layer.2.attention.q_lin.linear.weight: False\n",
      "distilbert.transformer.layer.2.attention.q_lin.linear.bias: False\n",
      "distilbert.transformer.layer.2.attention.q_lin.lora.W_a: True\n",
      "distilbert.transformer.layer.2.attention.q_lin.lora.W_b: True\n",
      "distilbert.transformer.layer.2.attention.k_lin.weight: False\n",
      "distilbert.transformer.layer.2.attention.k_lin.bias: False\n",
      "distilbert.transformer.layer.2.attention.v_lin.linear.weight: False\n",
      "distilbert.transformer.layer.2.attention.v_lin.linear.bias: False\n",
      "distilbert.transformer.layer.2.attention.v_lin.lora.W_a: True\n",
      "distilbert.transformer.layer.2.attention.v_lin.lora.W_b: True\n",
      "distilbert.transformer.layer.2.attention.out_lin.weight: False\n",
      "distilbert.transformer.layer.2.attention.out_lin.bias: False\n",
      "distilbert.transformer.layer.2.sa_layer_norm.weight: False\n",
      "distilbert.transformer.layer.2.sa_layer_norm.bias: False\n",
      "distilbert.transformer.layer.2.ffn.lin1.weight: False\n",
      "distilbert.transformer.layer.2.ffn.lin1.bias: False\n",
      "distilbert.transformer.layer.2.ffn.lin2.weight: False\n",
      "distilbert.transformer.layer.2.ffn.lin2.bias: False\n",
      "distilbert.transformer.layer.2.output_layer_norm.weight: False\n",
      "distilbert.transformer.layer.2.output_layer_norm.bias: False\n",
      "distilbert.transformer.layer.3.attention.q_lin.linear.weight: False\n",
      "distilbert.transformer.layer.3.attention.q_lin.linear.bias: False\n",
      "distilbert.transformer.layer.3.attention.q_lin.lora.W_a: True\n",
      "distilbert.transformer.layer.3.attention.q_lin.lora.W_b: True\n",
      "distilbert.transformer.layer.3.attention.k_lin.weight: False\n",
      "distilbert.transformer.layer.3.attention.k_lin.bias: False\n",
      "distilbert.transformer.layer.3.attention.v_lin.linear.weight: False\n",
      "distilbert.transformer.layer.3.attention.v_lin.linear.bias: False\n",
      "distilbert.transformer.layer.3.attention.v_lin.lora.W_a: True\n",
      "distilbert.transformer.layer.3.attention.v_lin.lora.W_b: True\n",
      "distilbert.transformer.layer.3.attention.out_lin.weight: False\n",
      "distilbert.transformer.layer.3.attention.out_lin.bias: False\n",
      "distilbert.transformer.layer.3.sa_layer_norm.weight: False\n",
      "distilbert.transformer.layer.3.sa_layer_norm.bias: False\n",
      "distilbert.transformer.layer.3.ffn.lin1.weight: False\n",
      "distilbert.transformer.layer.3.ffn.lin1.bias: False\n",
      "distilbert.transformer.layer.3.ffn.lin2.weight: False\n",
      "distilbert.transformer.layer.3.ffn.lin2.bias: False\n",
      "distilbert.transformer.layer.3.output_layer_norm.weight: False\n",
      "distilbert.transformer.layer.3.output_layer_norm.bias: False\n",
      "distilbert.transformer.layer.4.attention.q_lin.linear.weight: False\n",
      "distilbert.transformer.layer.4.attention.q_lin.linear.bias: False\n",
      "distilbert.transformer.layer.4.attention.q_lin.lora.W_a: True\n",
      "distilbert.transformer.layer.4.attention.q_lin.lora.W_b: True\n",
      "distilbert.transformer.layer.4.attention.k_lin.weight: False\n",
      "distilbert.transformer.layer.4.attention.k_lin.bias: False\n",
      "distilbert.transformer.layer.4.attention.v_lin.linear.weight: False\n",
      "distilbert.transformer.layer.4.attention.v_lin.linear.bias: False\n",
      "distilbert.transformer.layer.4.attention.v_lin.lora.W_a: True\n",
      "distilbert.transformer.layer.4.attention.v_lin.lora.W_b: True\n",
      "distilbert.transformer.layer.4.attention.out_lin.weight: False\n",
      "distilbert.transformer.layer.4.attention.out_lin.bias: False\n",
      "distilbert.transformer.layer.4.sa_layer_norm.weight: False\n",
      "distilbert.transformer.layer.4.sa_layer_norm.bias: False\n",
      "distilbert.transformer.layer.4.ffn.lin1.weight: False\n",
      "distilbert.transformer.layer.4.ffn.lin1.bias: False\n",
      "distilbert.transformer.layer.4.ffn.lin2.weight: False\n",
      "distilbert.transformer.layer.4.ffn.lin2.bias: False\n",
      "distilbert.transformer.layer.4.output_layer_norm.weight: False\n",
      "distilbert.transformer.layer.4.output_layer_norm.bias: False\n",
      "distilbert.transformer.layer.5.attention.q_lin.linear.weight: False\n",
      "distilbert.transformer.layer.5.attention.q_lin.linear.bias: False\n",
      "distilbert.transformer.layer.5.attention.q_lin.lora.W_a: True\n",
      "distilbert.transformer.layer.5.attention.q_lin.lora.W_b: True\n",
      "distilbert.transformer.layer.5.attention.k_lin.weight: False\n",
      "distilbert.transformer.layer.5.attention.k_lin.bias: False\n",
      "distilbert.transformer.layer.5.attention.v_lin.linear.weight: False\n",
      "distilbert.transformer.layer.5.attention.v_lin.linear.bias: False\n",
      "distilbert.transformer.layer.5.attention.v_lin.lora.W_a: True\n",
      "distilbert.transformer.layer.5.attention.v_lin.lora.W_b: True\n",
      "distilbert.transformer.layer.5.attention.out_lin.weight: False\n",
      "distilbert.transformer.layer.5.attention.out_lin.bias: False\n",
      "distilbert.transformer.layer.5.sa_layer_norm.weight: False\n",
      "distilbert.transformer.layer.5.sa_layer_norm.bias: False\n",
      "distilbert.transformer.layer.5.ffn.lin1.weight: False\n",
      "distilbert.transformer.layer.5.ffn.lin1.bias: False\n",
      "distilbert.transformer.layer.5.ffn.lin2.weight: False\n",
      "distilbert.transformer.layer.5.ffn.lin2.bias: False\n",
      "distilbert.transformer.layer.5.output_layer_norm.weight: False\n",
      "distilbert.transformer.layer.5.output_layer_norm.bias: False\n",
      "pre_classifier.weight: False\n",
      "pre_classifier.bias: False\n",
      "classifier.weight: False\n",
      "classifier.bias: False\n"
     ]
    }
   ],
   "source": [
    "# Check if linear layers are frozen\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8568a31d-bf0b-4343-9e60-0a66a3d3c0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters: 147456\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(\"Total number of trainable parameters:\", count_parameters(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946d29e3-6395-4846-bdbc-7c13eb3ca0f7",
   "metadata": {},
   "source": [
    "### 5) Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a79139-cc34-4feb-af03-bbad021e2434",
   "metadata": {},
   "source": [
    "**Wrap in LightningModule for Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a2cbbcf-7a2b-4428-8ee4-427eec1c0f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_model_utilities import CustomLightningModule\n",
    "\n",
    "lightning_model = CustomLightningModule(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5f8704e-dd84-4cae-a81e-a45be75bdffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        save_top_k=1, mode=\"max\", monitor=\"val_acc\"\n",
    "    )  # save top 1 model\n",
    "]\n",
    "logger = CSVLogger(save_dir=\"logs/\", name=\"my-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "719b6969-ba69-4f61-957e-03d9695077cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    max_epochs=3,\n",
    "    callbacks=callbacks,\n",
    "    accelerator=\"gpu\",\n",
    "    precision=\"16-mixed\",\n",
    "    devices=1,\n",
    "    logger=logger,\n",
    "    log_every_n_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8fc33052-1217-42f1-bb62-65020c039637",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                                | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | model    | DistilBertForSequenceClassification | 67.1 M | eval \n",
      "1 | val_acc  | MulticlassAccuracy                  | 0      | train\n",
      "2 | test_acc | MulticlassAccuracy                  | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "147 K     Trainable params\n",
      "67.0 M    Non-trainable params\n",
      "67.1 M    Total params\n",
      "268.410   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "96        Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                        | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5a9b074fe054b8d9d194ffa87889f65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed 9.34 min\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "trainer.fit(model=lightning_model,\n",
    "            train_dataloaders=train_loader,\n",
    "            val_dataloaders=val_loader)\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(f\"Time elapsed {elapsed/60:.2f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "17405173-57d7-49ba-9853-5152cff1670f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs/my-model/version_0/checkpoints/epoch=1-step=5834.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at logs/my-model/version_0/checkpoints/epoch=1-step=5834.ckpt\n",
      "/home/changjiang/miniconda3/envs/py310_pt220/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68ce9cbe995348e994ea54d505e85897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |                                                | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs/my-model/version_0/checkpoints/epoch=1-step=5834.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at logs/my-model/version_0/checkpoints/epoch=1-step=5834.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34c28df0bb0746bfbcf43fe6f075e7e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |                                                | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs/my-model/version_0/checkpoints/epoch=1-step=5834.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at logs/my-model/version_0/checkpoints/epoch=1-step=5834.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58997fe2fdba4f4e98cbc0fc44be1130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |                                                | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_acc = trainer.test(lightning_model, dataloaders=train_loader, ckpt_path=\"best\", verbose=False)\n",
    "val_acc = trainer.test(lightning_model, dataloaders=val_loader, ckpt_path=\"best\", verbose=False)\n",
    "test_acc = trainer.test(lightning_model, dataloaders=test_loader, ckpt_path=\"best\", verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a77cad8e-92b8-4ce3-b7b3-4928c1d314e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuning with LoRA \n",
      "**************\n",
      "Train acc: 93.58%\n",
      "Val acc:   90.18%\n",
      "Test acc:  89.63%\n",
      "********************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (\"Finetuning with LoRA \\n**************\")\n",
    "print(f\"Train acc: {train_acc[0]['accuracy']*100:2.2f}%\")\n",
    "print(f\"Val acc:   {val_acc[0]['accuracy']*100:2.2f}%\")\n",
    "print(f\"Test acc:  {test_acc[0]['accuracy']*100:2.2f}%\")\n",
    "print (\"********************************\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd32e5d7-b1c0-4540-81b4-16af895756b3",
   "metadata": {},
   "source": [
    "Cleanup checkpoint files as we don't need them later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "49723c50-d915-4524-9e07-d0cb7089d6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Cleanup checkpoint files as we don't need them later\n",
    "log_dir = f\"logs/my-model\"\n",
    "if os.path.exists(log_dir):\n",
    "    shutil.rmtree(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfce183f-96d6-4f85-9aba-5231b6cb9450",
   "metadata": {},
   "source": [
    "## 5. Comparison to Traditional Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec32a8d-874a-4477-ba45-a63f6ee4a336",
   "metadata": {},
   "source": [
    "In the previous section, we obtained 89.44% test accuracy with LoRA default settings. How does this compare to traditional finetuning?\n",
    "\n",
    "Let's start by training the DistilBERT model but only updating the last 2 layers during training. We can achieve this by first freezing all model weights and then unfreezing the two linear output layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "edf84a9e-61ce-477c-aa0f-528bf947455e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "del model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c723cae-4342-4c7d-bdf6-2d3f244c9b88",
   "metadata": {},
   "source": [
    "**Freeze all layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e3e526d5-371a-4446-a0cc-641a2fdffd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters: 66955010\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of trainable parameters:\", count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "702f232b-3cf0-49b9-b248-67a171bc9fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755a5508-da35-4309-899f-55a21e6a44dc",
   "metadata": {},
   "source": [
    "**Unfreeze last layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b9c6a261-0be2-4730-b163-167ae5f93a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input model \n",
      "**************\n",
      " DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print (\"Input model \\n**************\\n\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c498c47b-7b97-4f47-af32-481620bf946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.pre_classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7f5e3ce9-f516-4af1-8428-32d8733656fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters: 592130\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of trainable parameters:\", count_parameters(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96a69f8-42eb-4baf-ba6f-349d8411ae94",
   "metadata": {},
   "source": [
    "### Finetuning\n",
    "- Wrap in LightningModule for Training, similar to the code shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6ac0551c-00af-4f0d-96b4-261001afdadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from local_model_utilities import CustomLightningModule\n",
    "\n",
    "lightning_model = CustomLightningModule(model)\n",
    "\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        save_top_k=1, mode=\"max\", monitor=\"val_acc\"\n",
    "    )  # save top 1 model\n",
    "]\n",
    "logger = CSVLogger(save_dir=\"logs/\", name=\"my-model\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=3,\n",
    "    callbacks=callbacks,\n",
    "    accelerator=\"gpu\",\n",
    "    precision=\"16-mixed\",\n",
    "    devices=1,\n",
    "    logger=logger,\n",
    "    log_every_n_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ddace0fb-3d72-492d-9ee7-9fee9eac33a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                                | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | model    | DistilBertForSequenceClassification | 67.0 M | eval \n",
      "1 | val_acc  | MulticlassAccuracy                  | 0      | train\n",
      "2 | test_acc | MulticlassAccuracy                  | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "592 K     Trainable params\n",
      "66.4 M    Non-trainable params\n",
      "67.0 M    Total params\n",
      "267.820   Total estimated model params size (MB)\n",
      "2         Modules in train mode\n",
      "96        Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                        | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b1e211992e84ab18a9af341d9f8f6b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed 4.29 min\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "trainer.fit(model=lightning_model,\n",
    "            train_dataloaders=train_loader,\n",
    "            val_dataloaders=val_loader)\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(f\"Time elapsed {elapsed/60:.2f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f416ddc3-63cc-42aa-a8e8-e47edabde494",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs/my-model/version_0/checkpoints/epoch=2-step=8751.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at logs/my-model/version_0/checkpoints/epoch=2-step=8751.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "739f68b0529a4b1eb965405d78cf54ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |                                                | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_acc = trainer.test(lightning_model, dataloaders=test_loader, ckpt_path=\"best\", verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "215e532b-571c-4445-92a9-be04124b42f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finetuning with last 2 layers\n",
      "****************\n",
      "Train acc: 93.58%\n",
      "Val acc:   90.18%\n",
      "Test acc:  86.45%\n",
      "****************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (\"finetuning with last 2 layers\\n****************\")\n",
    "print(f\"Train acc: {train_acc[0]['accuracy']*100:2.2f}%\")\n",
    "print(f\"Val acc:   {val_acc[0]['accuracy']*100:2.2f}%\")\n",
    "print(f\"Test acc:  {test_acc[0]['accuracy']*100:2.2f}%\")\n",
    "print (\"****************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c745f76c-ee00-40c5-ae09-20c8e5a3a79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Cleanup checkpoint files as we don't need them later\n",
    "log_dir = f\"logs/my-model\"\n",
    "if os.path.exists(log_dir):\n",
    "    shutil.rmtree(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8200559-adf7-4862-b97e-ee4ad7383502",
   "metadata": {},
   "source": [
    "So, as we can see, finetuning all layers, which involves training 66,955,010 parameters, performs better than both finetuning only the last two layers (592,130 parameters) and the LoRA defaults (147,456 parameters).\n",
    "\n",
    "As a takeaway message, so far, LoRA performs better than the conventional finetuning of the last two layers even though it uses 4x fewer parameters. Finetuning all layers requires updating 450x more parameters than in the LoRA setting but also results in 2% higher test accuracy. \n",
    "\n",
    "However, one aspect to consider is that we only used LoRA default settings so far. Perhaps we can bridge the gap between the full finetuning and LoRA finetuning with a different LoRA hyperparameter configuration? We will answer this question in the upcoming section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8270e6f-42e8-4eae-a343-ca40e080bc66",
   "metadata": {},
   "source": [
    "## 6. Optimizing the LoRA Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0754f675-5d68-49fa-98ad-4699cb6cf2b3",
   "metadata": {},
   "source": [
    "The previous LoRA finetuning results were based on the following hyperparameter choices:\n",
    "\n",
    "```python\n",
    "lora_r = 8\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.05\n",
    "lora_query = True\n",
    "lora_key = False\n",
    "lora_value = True\n",
    "lora_projection = False\n",
    "lora_mlp = False\n",
    "lora_head = False\n",
    "```\n",
    "\n",
    "Note that this only involves applying LoRA to an attention layer's query and value weight matrices. Optionally, we can enable LoRA for other layers as well. Furthermore, we can control the number of trainable parameters in each LoRA layer by modifying the rank (`lora_r`). \n",
    "\n",
    "To try different hyperparameter configurations, you can use our compact `finetune-lora-script.py` script, which accepts the hyperparameter choices as command line arguments:\n",
    "\n",
    "```bash\n",
    "python finetune-lora-script.py --lora_alpha 32 --lora_r 16\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610d5b39-16d1-4dd1-90b6-c6f0b1d5992a",
   "metadata": {},
   "source": [
    "In addition, you can also toggle other hyperparameter settings, for example:\n",
    "\n",
    "```bash\n",
    "python 03_finetune-lora.py \\\n",
    "--lora_alpha 32 \\\n",
    "--lora_r 16 \\\n",
    "--lora_query True \\\n",
    "--lora_key True \\\n",
    "--lora_value True \\\n",
    "--lora_projection True \\\n",
    "--lora_mlp True \\\n",
    "--lora_head True\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf4a5bb-faa9-435c-a298-ed6826eb96f2",
   "metadata": {},
   "source": [
    "One way to improve the LoRA performance is to manually fiddle with these hyperparameter choices. However, to make the hyperparameter tuning more convenient, you can also use the `gridsearch.py` script, which runs the following hyperparameter grid on all available GPUs:\n",
    "\n",
    "```python\n",
    "alpha_values = [1, 4, 8, 16, 32, 64]\n",
    "rank_values = [1, 2, 4, 8, 16, 32]\n",
    "devices = range(torch.cuda.device_count())\n",
    "lora_query = [\"True\"]\n",
    "lora_key = [\"False\", \"True\"]\n",
    "lora_value = [\"True\"]\n",
    "lora_projection = [\"False\", \"True\"]\n",
    "lora_mlp = [\"False\", \"True\"]\n",
    "lora_head = [\"False\", \"True\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3b46ad-682e-4b9e-b378-426b9bf2cf75",
   "metadata": {},
   "source": [
    "The way the `finetune-lora-script.py` script is set up, the results are saved to a `results.txt file`. Inspecting the `results.txt` file after the run is completed, the best hyperparameter configurations seem to be the following:\n",
    "\n",
    "```plain\n",
    "lora_r: 8\n",
    "lora_alpha: 1\n",
    "lora_query: True\n",
    "lora_key: False\n",
    "lora_value: True\n",
    "lora_projection: False\n",
    "lora_mlp: True\n",
    "lora_head: False\n",
    "```\n",
    "\n",
    "Resulting in:\n",
    "-   Val acc: 92.96%\n",
    "-   Test acc: 92.39%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3ceb9a-9e44-46e2-b3c9-8df4854d0fc1",
   "metadata": {},
   "source": [
    "Let us run the grid search code. It will actually call the python scirpt `finetune-lora-script.py` as shown in the function `run_script()` defined in the `gridsearch.py`.\n",
    "\n",
    "```python\n",
    "def run_script(alpha, rank, device, query, key, value, projection, mlp, head):\n",
    "    global device_usage\n",
    "    # it will call the python srcipt `finetune-lora-script.py`;\n",
    "    command = [\n",
    "        'python3', 'finetune-lora-script.py',\n",
    "        '--lora_alpha', str(alpha),\n",
    "        '--lora_r', str(rank),\n",
    "        '--device', str(device),\n",
    "        '--lora_query', query,\n",
    "        '--lora_key', key,\n",
    "        '--lora_value', value,\n",
    "        '--lora_projection', projection,\n",
    "        '--lora_mlp', mlp,\n",
    "        '--lora_head', head,\n",
    "        '--verbose', \"False\"\n",
    "    ]\n",
    "\n",
    "    print(f\"Starting run with alpha = {alpha}, rank = {rank}, lora_query = {query}, lora_key = {key}, lora_value = {value}, lora_projection = {projection}, lora_mlp = {mlp}, lora_head = {head} on device {device}\")\n",
    "    subprocess.run(command)\n",
    "    print(f\"Completed run with alpha = {alpha}, rank = {rank}, lora_query = {query}, lora_key = {key}, lora_value = {value}, lora_projection = {projection}, lora_mlp = {mlp}, lora_head = {head} on device {device}\")\n",
    "\n",
    "    # Mark the device as no longer in use\n",
    "    device_usage[device] = False\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db6d192-3de5-4552-9c6e-bb1cc80b33a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

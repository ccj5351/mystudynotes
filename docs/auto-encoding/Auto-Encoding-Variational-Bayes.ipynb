{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-Encoding Variational Bayes\n",
    "\n",
    "> see the paper [Auto-Encoding Variational Bayes](https://arxiv.org/pdf/1312.6114.pdf), Diederik P Kingma, Max Welling, ICLR 2014\n",
    "\n",
    "> see slide 1: http://lear.inrialpes.fr/~verbeek/tmp/AEVB.jjv.pdf\n",
    "\n",
    "\n",
    "> see slide 2: https://www.slideshare.net/mehdidc/auto-encodingvariationalbayes-54478304\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a generative model ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A model of how the data $X$ was generated\n",
    "- Typically, the purpose is to find a model for: $p(x)$ or $p(x, y)$\n",
    "- y can be a set of latent (hidden) variables or a set of output variables, for discriminative problems\n",
    "- Note: `latent variables`, as opposed to observable variables, are variables that are not directly observed but are rather inferred (through a mathematical model) from other variables that are observed (directly measured). Mathematical models that aim to explain observed variables in terms of latent variables are called `latent variable models`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training generative models\n",
    "Typically, we assume a parametric form of the probability density: \n",
    "$$p ( x | \\theta)$$\n",
    "\n",
    "Given an i.i.d dataset: $X = ( x_1 , x_2 , ..., x_N )$ , we typically do:\n",
    "- Maximum likelihood (ML) :  $$\\operatorname*{argmax}_\\theta p( X | \\theta)$$\n",
    "- Maximum a posterior (MAP) : $$ \\operatorname*{argmax}_\\theta p ( X | \\theta) p(\\theta)$$\n",
    "- Bayesian inference : $$p (\\theta | X) = \\frac{p (x | \\theta) p (\\theta)} { \\int_ \\theta p (x | \\theta) p (\\theta) d\\theta}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem\n",
    "\n",
    "- let $x$ be the observed variables\n",
    "- we assume a latent representation $z$ (Note: again, latent variables are variables that are not directly observed but are rather inferred (through a mathematical model) from other variables that are observed (directly measured))\n",
    "- we define $p_\\theta(z)$ and $p_\\theta ( x | z )$\n",
    "\n",
    "We want to design a generative model where: \n",
    "- **marginal** $p_\\theta( x ) = \\int p_\\theta( x | z ) p_\\theta ( z) dz$  is intractable\n",
    "- **posterior** $p_\\theta(z|x) = \\frac{p_\\theta (x|z) p_\\theta(z)}{p_\\theta(x)}$ is intractable\n",
    "- we have **large datasets**: we want to avoid sampling based training procedures (e.g., MCMC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The proposed solution\n",
    "\n",
    "They propose:\n",
    "- a fast training procedure that estimates the parameters $\\theta$: for **data generation**\n",
    "\n",
    "- an approximation of the posterior $p_\\theta (z | x) $ : for **data representation**\n",
    "\n",
    "- an approximation of the marginal $p_\\theta( x )$ : for **model\n",
    "evaluation and as a prior for other tasks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulation of the problem\n",
    "\n",
    "The process of generation consists of sampling $z$ from $p_\\theta( x | z )$.\n",
    "\n",
    "Let's define:\n",
    "- a prior over the latent representation $p_\\theta(z)$,\n",
    "- a **decoder**  $p_\\theta( x | z )$\n",
    "\n",
    "We want to maximize the log-likelihood of the data $( x^{( 1 )} , x^{ ( 2 )} , \\dots, x^{( N )})$:\n",
    "\n",
    "$$\\log p_\\theta ( x^{( 1 )} , x^{ ( 2 )} , \\dots, x^{( N )}) = \\sum_i \\log p_\\theta(x_i)$$\n",
    "\n",
    "and be able to do inference: $p_\\theta(z | x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The variational lower bound\n",
    "\n",
    "\n",
    "<img src=\"../files/auto-encoding-variational-bayes-fig1.png\" alt=\"drawing\" width=\"700\"/>\n",
    "\n",
    "- We will learn an approximate of the intractable posterior $p_\\theta ( z | x )$ : $q_\\phi ( z | x )$ by maximizing a lower bound of the log-likelihood of the data\n",
    "\n",
    "- We can write :\n",
    "\n",
    "$$\\log p_\\theta (x) = D_{KL}( q_\\phi ( z|x) || p_\\theta(z|x)) + L(\\theta, \\phi, x )$$ where:\n",
    "\n",
    "$$L(\\theta, \\phi, x ) = \\mathbb{E}_{q_\\phi ( z|x)} [ \\log p_\\theta( x, z ) − \\log q_\\phi( z | x)] $$\n",
    "\n",
    "- $L(\\theta, \\phi, x )$ is called the **variational lower bound**, and the goal is to maximize it w.r.t to all the parameters $(\\theta, \\phi)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the lower bound gradients\n",
    "\n",
    "- We need to compute $\\frac{\\partial L(\\theta, \\phi, x )}{\\partial \\theta}$ and $\\frac{\\partial L(\\theta, \\phi, x )}{\\partial \\phi}$ to apply gradient descent\n",
    "\n",
    "- For that, we use the **reparametrisation trick** : we sample from a noise variable $p(\\epsilon)$ and apply a determenistic function to it so that we obtain correct samples from $q_\\phi ( z | x )$, meaning:\n",
    "\n",
    " - if $ \\epsilon \\sim p(\\epsilon)$ we find $g$ so that if $z = g(x, \\phi, \\epsilon)$ then $z \\sim q_\\phi (z | x)$\n",
    " \n",
    " - $g$ can be the **inverse CDF** of $q_\\phi ( z | x )$ if $\\epsilon$ is uniform\n",
    "\n",
    "- With the reparametrisation trick we can rewrite L:\n",
    "$$L(\\theta, \\phi, x ) = \\mathbb{E}_{ \\epsilon \\sim p(\\epsilon)} [\\log p_\\theta( x, g(x, \\phi, \\epsilon) ) − \\log q_\\phi ( g(x, \\phi, \\epsilon) | x)]$$\n",
    "\n",
    "- We then estimate the gradients with **Monte Carlo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A connection with auto-encoders\n",
    "\n",
    "- Note that $L$ can also be written in this form:\n",
    "\n",
    "$$ L(\\theta, \\phi, x ) = - D_{KL} (q_\\phi ( z | x ) || p_\\theta (z)) + \n",
    "\\mathbb{E}_{q_\\phi( z|x)} [\\log p_\\theta(x | z)]$$\n",
    "\n",
    "- We can interpret the first term as a **regularizer**: it forces\n",
    "$q_\\theta( z | x )$ to not be too divergent from the prior $p_\\theta (z)$\n",
    "\n",
    "- We can interpret the second term as the **reconstruction error**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The algorithm\n",
    "\n",
    "<img src=\"../files/auto-encoding-variational-bayes-algr1.png\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoders (VAEs)\n",
    "\n",
    "It is a model example which uses the procedure described above to maximize the lower bound\n",
    "\n",
    "In VAEs, we choose:\n",
    "- $p_\\theta (z) = N ( 0 , \\mathbf {I} )$ \n",
    "- $p_\\theta ( x | z )$ : \n",
    " - is normal distribution for real data, we have **neural network decoder** that computes $\\mu$ and $\\sigma$ of this distribution from $z$\n",
    " - is multivariate bernoulli for boolean data, we have **neural network decoder** that computes the probability of 1 from $z$\n",
    "\n",
    "- $q_\\phi ( z | x ) = N \\left( \\mu( x ), \\sigma(x) \\mathbf {I}\\right) $: we have a **neural network encoder** that computes $\\mu$ and $\\sigma$ of $q_\\phi(z | x )$ from $x$\n",
    "- $ \\epsilon \\sim N ( 0 , \\mathbf {I} )$ and $z = g(x, \\phi, \\epsilon) = \\mu(x) + \\sigma(x)∗ \\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

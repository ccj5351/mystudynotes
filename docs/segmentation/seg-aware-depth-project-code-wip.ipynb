{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding and Debugging Logs for Segmentation-Depth PRoject\n",
    "\n",
    "This document will record the whole story how I do this project in terms of coding and core ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Logs\n",
    "- 2019/03/12/: Error: `Shape must be rank 0 but is rank 1 for 'BatchDataset' (op: 'BatchDataset') with input shapes: [], [1].` It is due to the ',' in `self.batch_size = batch_size,`. I changed it to 'self.batch_size = batch_size';\n",
    "- UserWarning: An unusually high number of `Iterator.get_next()` calls was detected. This often indicates that `Iterator.get_next()` is being called inside a training loop, which will cause gradual slowdown and eventual resource exhaustion. If this is the case, restructure your code to call `next_element = iterator.get_next()` once outside the loop, and use `next_element` as the input to some computation that is invoked inside the loop. warnings.warn(GET_NEXT_CALL_WARNING_MESSAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customed Caffe layers: im2parity, im2dist and DistLoss\n",
    "The following is the Caffe layers defined in [Segmentation-Aware Convolutional Networks Using Local Attention Masks](https://github.com/aharley/segaware) project. Let us give detailed analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) im2col_layer in `~/seg-depth/segaware-master/caffe/src/caffe/layers/im2col_layer.cu`:\n",
    "```cpp\n",
    "#include <vector>\n",
    "\n",
    "#include \"caffe/layers/im2col_layer.hpp\"\n",
    "#include \"caffe/util/im2col.hpp\"\n",
    "\n",
    "namespace caffe {\n",
    "\n",
    "template <typename Dtype>\n",
    "void Im2colLayer<Dtype>::Forward_gpu(const vector<Blob<Dtype>*>& bottom,\n",
    "      const vector<Blob<Dtype>*>& top) {\n",
    "  const Dtype* bottom_data = bottom[0]->gpu_data();\n",
    "  Dtype* top_data = top[0]->mutable_gpu_data();\n",
    "  const int num_kernels = channels_ * top[0]->count(channel_axis_ + 1);\n",
    "  for (int n = 0; n < num_; ++n) {\n",
    "    if (!force_nd_im2col_ && num_spatial_axes_ == 2) {\n",
    "      im2col_gpu(bottom_data + n * bottom_dim_, channels_,\n",
    "          bottom[0]->shape(channel_axis_ + 1),\n",
    "          bottom[0]->shape(channel_axis_ + 2),\n",
    "          kernel_shape_.cpu_data()[0], kernel_shape_.cpu_data()[1],\n",
    "          pad_.cpu_data()[0], pad_.cpu_data()[1],\n",
    "          stride_.cpu_data()[0], stride_.cpu_data()[1],\n",
    "          dilation_.cpu_data()[0], dilation_.cpu_data()[1],\n",
    "          top_data + n * top_dim_);\n",
    "    } else {\n",
    "      im2col_nd_gpu(bottom_data + n * bottom_dim_, num_spatial_axes_,\n",
    "          num_kernels, bottom[0]->gpu_shape() + channel_axis_,\n",
    "          top[0]->gpu_shape() + channel_axis_,\n",
    "          kernel_shape_.gpu_data(), pad_.gpu_data(), stride_.gpu_data(),\n",
    "          dilation_.gpu_data(), top_data + n * top_dim_);\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "template <typename Dtype>\n",
    "void Im2colLayer<Dtype>::Backward_gpu(const vector<Blob<Dtype>*>& top,\n",
    "      const vector<bool>& propagate_down, const vector<Blob<Dtype>*>& bottom) {\n",
    "  const Dtype* top_diff = top[0]->gpu_diff();\n",
    "  Dtype* bottom_diff = bottom[0]->mutable_gpu_diff();\n",
    "  for (int n = 0; n < num_; ++n) {\n",
    "    if (!force_nd_im2col_ && num_spatial_axes_ == 2) {\n",
    "      col2im_gpu(top_diff + n * top_dim_, channels_,\n",
    "          bottom[0]->shape(channel_axis_ + 1),\n",
    "          bottom[0]->shape(channel_axis_ + 2),\n",
    "          kernel_shape_.cpu_data()[0], kernel_shape_.cpu_data()[1],\n",
    "          pad_.cpu_data()[0], pad_.cpu_data()[1],\n",
    "          stride_.cpu_data()[0], stride_.cpu_data()[1],\n",
    "          dilation_.cpu_data()[0], dilation_.cpu_data()[1],\n",
    "          bottom_diff + n * bottom_dim_);\n",
    "    } else {\n",
    "      col2im_nd_gpu(top_diff + n * top_dim_, num_spatial_axes_, bottom_dim_,\n",
    "          bottom[0]->gpu_shape() + channel_axis_,\n",
    "          top[0]->gpu_shape() + channel_axis_,\n",
    "          kernel_shape_.gpu_data(), pad_.gpu_data(), stride_.gpu_data(),\n",
    "          dilation_.gpu_data(), bottom_diff + n * bottom_dim_);\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "INSTANTIATE_LAYER_GPU_FUNCS(Im2colLayer);\n",
    "\n",
    "}  // namespace caffe\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) im2parity_layer in `~/seg-depth/segaware-master/caffe/src/caffe/layers/im2parity_layer.cu`:\n",
    "```cpp\n",
    "#include <vector>\n",
    "\n",
    "#include \"caffe/layers/im2parity_layer.hpp\"\n",
    "#include \"caffe/util/im2parity.hpp\"\n",
    "\n",
    "namespace caffe {\n",
    "\n",
    "template <typename Dtype>\n",
    "void Im2parityLayer<Dtype>::Forward_gpu(const vector<Blob<Dtype>*>& bottom,\n",
    "\t\t\t\t      const vector<Blob<Dtype>*>& top) {\n",
    "  const Dtype* bottom_data = bottom[0]->gpu_data();\n",
    "  Dtype* top_data = top[0]->mutable_gpu_data();\n",
    "  const int num_kernels = channels_ * top[0]->count(channel_axis_ + 1);\n",
    "  for (int n = 0; n < num_; ++n) {\n",
    "    im2parity_gpu(bottom_data + n * bottom_dim_, channels_,\n",
    "  \t\tbottom[0]->shape(channel_axis_ + 1),\n",
    "  \t\tbottom[0]->shape(channel_axis_ + 2),\n",
    "  \t\tkernel_shape_.cpu_data()[0], kernel_shape_.cpu_data()[1],\n",
    "  \t\tpad_.cpu_data()[0], pad_.cpu_data()[1],\n",
    "  \t\tstride_.cpu_data()[0], stride_.cpu_data()[1],\n",
    "  \t\tdilation_.cpu_data()[0], dilation_.cpu_data()[1],\n",
    "  \t\thas_ignore_label_, ignore_label_,\n",
    "  \t\ttop_data + n * top_dim_);\n",
    "  }\n",
    "  // const Dtype* okok = bottom[0]->cpu_data();\n",
    "  // for (int i=0; i < 12; ++i)\n",
    "  //   LOG(ERROR) << \"bottom_data[\" << i << \"] = \" << okok[i];\n",
    "  // const Dtype* okok = top[0]->cpu_data();\n",
    "  // for (int i=0; i < top[0]->count(); ++i)\n",
    "  //   LOG(ERROR) << \"top_data[\" << i << \"] = \" << okok[i];\n",
    "  // const Dtype* okok = diff_.cpu_data();\n",
    "  // for (int i=0; i < diff_.count(); ++i)\n",
    "  //   LOG(ERROR) << \"diff_data[\" << i << \"] = \" << okok[i];\n",
    "  // LOG(ERROR) << \"done forward!!\" << std::endl;\n",
    "\n",
    "}\n",
    "\n",
    "template <typename Dtype>\n",
    "void Im2parityLayer<Dtype>::Backward_gpu(const vector<Blob<Dtype>*>& top,\n",
    "      const vector<bool>& propagate_down, const vector<Blob<Dtype>*>& bottom) {\n",
    "  NOT_IMPLEMENTED;\n",
    "}\n",
    "\n",
    "\n",
    "INSTANTIATE_LAYER_GPU_FUNCS(Im2parityLayer);\n",
    "\n",
    "}  // namespace caffe\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) im2dist_layer in `~/seg-depth/segaware-master/caffe/src/caffe/layers/im2dist_layer.cu`:\n",
    "\n",
    "```cpp\n",
    "#include <vector>\n",
    "#include \"caffe/layers/im2dist_layer.hpp\"\n",
    "#include \"caffe/util/im2dist.hpp\"\n",
    "\n",
    "namespace caffe {\n",
    "\n",
    "template <typename Dtype>\n",
    "void Im2distLayer<Dtype>::Forward_gpu(const vector<Blob<Dtype>*>& bottom,\n",
    "\t\t\t\t      const vector<Blob<Dtype>*>& top) {\n",
    "  const Dtype* bottom_data = bottom[0]->gpu_data();\n",
    "  Dtype* top_data = top[0]->mutable_gpu_data();\n",
    "  Dtype* diff_data = diff_.mutable_gpu_data();\n",
    "  const int num_kernels = channels_ * top[0]->count(channel_axis_ + 1);\n",
    "  for (int n = 0; n < num_; ++n) {\n",
    "    im2dist_gpu(bottom_data + n * bottom_dim_, channels_,\n",
    "\t\tbottom[0]->shape(channel_axis_ + 1),\n",
    "\t\tbottom[0]->shape(channel_axis_ + 2),\n",
    "\t\tkernel_shape_.cpu_data()[0], kernel_shape_.cpu_data()[1],\n",
    "\t\tpad_.cpu_data()[0], pad_.cpu_data()[1],\n",
    "\t\tstride_.cpu_data()[0], stride_.cpu_data()[1],\n",
    "\t\tdilation_.cpu_data()[0], dilation_.cpu_data()[1],\n",
    "\t\ttop_data + n * top_dim_,\n",
    "\t\tdiff_data + n * diff_dim_, norm_,\n",
    "\t\tremove_center_, remove_bounds_);\n",
    "  }\n",
    "  // const Dtype* embs = top[0]->cpu_data();\n",
    "  // // for (int i=190; i < 210; i++){\n",
    "  // for (int i=0; i < top[0]->count(); i++) {\n",
    "  //   LOG(ERROR) << \"for example, im2dist[\" << i << \"] = \" << embs[i];\n",
    "  // }\n",
    "}\n",
    "\n",
    "template <typename Dtype>\n",
    "void Im2distLayer<Dtype>::Backward_gpu(const vector<Blob<Dtype>*>& top,\n",
    "      const vector<bool>& propagate_down, const vector<Blob<Dtype>*>& bottom) {\n",
    "  const Dtype* top_diff = top[0]->gpu_diff();\n",
    "  const Dtype* diff_data = diff_.gpu_data();\n",
    "  Dtype* bottom_diff = bottom[0]->mutable_gpu_diff();\n",
    "  for (int n = 0; n < num_; ++n) {\n",
    "    dist2im_gpu(top_diff + n * top_dim_, \n",
    "  \t\tdiff_data + n * diff_dim_, \n",
    "  \t\tchannels_,\n",
    "  \t\tbottom[0]->shape(channel_axis_ + 1),\n",
    "  \t\tbottom[0]->shape(channel_axis_ + 2),\n",
    "  \t\tkernel_shape_.cpu_data()[0], kernel_shape_.cpu_data()[1],\n",
    "  \t\tpad_.cpu_data()[0], pad_.cpu_data()[1],\n",
    "  \t\tstride_.cpu_data()[0], stride_.cpu_data()[1],\n",
    "  \t\tdilation_.cpu_data()[0], dilation_.cpu_data()[1],\n",
    "  \t\tbottom_diff + n * bottom_dim_, norm_,\n",
    "\t\tremove_center_, remove_bounds_);\n",
    "  }\n",
    "}\n",
    "\n",
    "INSTANTIATE_LAYER_GPU_FUNCS(Im2distLayer);\n",
    "\n",
    "}  // namespace caffe\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) dist_loss_gpu_kernel() in `~/seg-depth/segaware-master/caffe/src/caffe/layers/dist_loss_layer.cu`:\n",
    "```cpp\n",
    "#include <vector>\n",
    "#include \"caffe/layers/dist_loss_layer.hpp\"\n",
    "#include \"caffe/util/math_functions.hpp\"\n",
    "\n",
    "namespace caffe {\n",
    "\n",
    "// void dist_loss_gpu_kernel(const int n, const Dtype* dist_col, const Dtype* parity_col,\n",
    "template <typename Dtype>\n",
    "__global__ void dist_loss_gpu_kernel(const int n, const Dtype* dist_col, const Dtype* parity_col,\n",
    "    const int height_col, const int width_col, const int channels_col, \n",
    "    const bool has_ignore_label, const Dtype ignore_label, \n",
    "    const Dtype alpha, const Dtype beta, \n",
    "    Dtype* diff_col) {\n",
    "  // for (int index = 0; index < n; ++index) {\n",
    "  CUDA_KERNEL_LOOP(index, n) {\n",
    "    int w_out = index % width_col;\n",
    "    int h_index = index / width_col;\n",
    "    int h_out = h_index % height_col;\n",
    "    const Dtype* dist_col_ptr = dist_col;\n",
    "    const Dtype* parity_col_ptr = parity_col;\n",
    "    Dtype* diff_col_ptr = diff_col;\n",
    "    dist_col_ptr += h_out * width_col + w_out;\n",
    "    parity_col_ptr += h_out * width_col + w_out;\n",
    "    diff_col_ptr += h_out * width_col + w_out;\n",
    "    for (int i = 0; i < channels_col; ++i) {\n",
    "      const Dtype dist =  *dist_col_ptr;\n",
    "      int parity = *parity_col_ptr;\n",
    "      if (has_ignore_label && parity==ignore_label) {\n",
    "\tcontinue;\n",
    "      } else {\n",
    "\tif (parity)\n",
    "\t  *diff_col_ptr = max(dist-alpha, Dtype(0));\n",
    "\telse\n",
    "\t  *diff_col_ptr = -max(beta-dist, Dtype(0));\n",
    "      }\n",
    "      dist_col_ptr += height_col * width_col;\n",
    "      parity_col_ptr += height_col * width_col;\n",
    "      diff_col_ptr += height_col * width_col;\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "template <typename Dtype>\n",
    "void DistLossLayer<Dtype>::Forward_gpu(const vector<Blob<Dtype>*>& bottom,\n",
    "      const vector<Blob<Dtype>*>& top) {\n",
    "  int num = bottom[0]->num();\n",
    "  int height_col = bottom[0]->height();\n",
    "  int width_col = bottom[0]->width();\n",
    "  int channels_col = bottom[0]->channels();\n",
    "  int count = bottom[0]->count();\n",
    "  // start one kernel per position, then within go through the channels\n",
    "  int num_kernels = height_col * width_col;\n",
    "\n",
    "  const Dtype* dist_col = bottom[0]->gpu_data();\n",
    "  const Dtype* parity_col = bottom[1]->gpu_data();\n",
    "  Dtype* diff_col = diff_.mutable_gpu_data();\n",
    "  Dtype loss = 0;\n",
    "  caffe_gpu_set(height_col * width_col * channels_col, Dtype(0), diff_col);\n",
    "  // NOLINT_NEXT_LINE(whitespace/operators)\n",
    "  dist_loss_gpu_kernel<Dtype><<<CAFFE_GET_BLOCKS(num_kernels),\n",
    "                                CAFFE_CUDA_NUM_THREADS>>>(\n",
    "      num_kernels, dist_col, parity_col, \n",
    "      height_col, width_col, channels_col, \n",
    "      has_ignore_label_, ignore_label_, \n",
    "      alpha_, beta_, \n",
    "      diff_col);\n",
    "  CUDA_POST_KERNEL_CHECK;\n",
    "  caffe_gpu_asum(count,diff_col,&loss);\n",
    "  const Dtype* dist_cpu = diff_.cpu_data();\n",
    "  top[0]->mutable_cpu_data()[0] = loss / count;\n",
    "}\n",
    "\n",
    "template <typename Dtype>\n",
    "void DistLossLayer<Dtype>::Backward_gpu(const vector<Blob<Dtype>*>& top,\n",
    "      const vector<bool>& propagate_down, const vector<Blob<Dtype>*>& bottom) {\n",
    "  int count = bottom[0]->count();\n",
    "  Dtype* bottom_diff = bottom[0]->mutable_gpu_diff();\n",
    "  const Dtype* diff_col = diff_.gpu_data();\n",
    "  caffe_copy(count, diff_col, bottom_diff);\n",
    "  caffe_gpu_scal(count, Dtype(1) / count, bottom_diff);\n",
    "\n",
    "  // Dtype* bottom_diff = bottom[0]->mutable_cpu_diff();\n",
    "  // const Dtype* diff_col = diff_.cpu_data();\n",
    "  // caffe_copy(count, diff_col, bottom_diff);\n",
    "  // caffe_scal(count, Dtype(1) / num / height_col / width_col, bottom_diff);\n",
    "}\n",
    "\n",
    "INSTANTIATE_LAYER_GPU_FUNCS(DistLossLayer);\n",
    "\n",
    "}  // namespace caffe\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

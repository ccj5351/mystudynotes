{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80b253b1-64ac-4fc7-aadd-73fff405f6c6",
   "metadata": {},
   "source": [
    "# Embedding layer vs Linear Layer in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715206d3-ae23-47fc-a1db-416e41c7334f",
   "metadata": {},
   "source": [
    "> See this question at  [What is the difference between an Embedding Layer with a bias immediately afterwards and a Linear Layer in PyTorch](https://stackoverflow.com/questions/65445174/what-is-the-difference-between-an-embedding-layer-with-a-bias-immediately-afterw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de4a3de-b29c-4830-9091-bd8f4f3a2672",
   "metadata": {},
   "source": [
    "## Embedding Layer Syntax\n",
    "\n",
    "Syntax: \n",
    "```python\n",
    "torch.nn.Embedding(num_embeddings, embedding_dim, \n",
    "                   padding_idx=None, max_norm=None,...)\n",
    "```\n",
    "                \n",
    "A simple lookup table that stores embeddings of a fixed dictionary and size.\n",
    "\n",
    "This module is often used to store word embeddings and retrieve them using indices. The input to the module is  a list of indices, and the output is the corresponding word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cc4f91-9249-44f2-b7b8-260cade576b6",
   "metadata": {},
   "source": [
    "## Answer 1:\n",
    "\n",
    "Both implement a linear transformation of the input data, however  `nn.Embedding`  allow to pass an index-based objects, while  `nn.Linear`  requires you to encode them using one-hot encoding. I.e. let's say you have a vocabulary of size 16, with characters encoded by numbers from 0 to 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c91803f8-0756-4d22-9400-6f85af25ef83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "model = nn.Embedding(num_embeddings=16, embedding_dim=256)\n",
    "x = torch.tensor([0, 4, 7]) # 3 characters\n",
    "emb = model(x)\n",
    "emb.shape #>> (3, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db6300e-5e8b-49d3-aef6-14996e42c831",
   "metadata": {},
   "source": [
    "So essentially, each of your input characters were mapped to vector of 256 floats. Because of that, people refer to  `nn.Embedding`  as a look-up table, which might be confusing, \n",
    "if you think of a look-up table as something frozen, but it is not, the parameters of  `nn.Embedding`  are trainable.\n",
    "\n",
    "To achieve similar result with  `nn.Linear`, you will need to first encode your input `x` using one-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c96ab8b0-ccc4-4fd3-89d4-b7bed1ce7945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Linear(in_features=16, out_features=256, bias=False)\n",
    "x = torch.tensor([0, 4, 7]) # 3 characters\n",
    "# emb = model(x)\n",
    "#>> RuntimeError: RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x3 and 16x256)\n",
    "\n",
    "x_enc = torch.eye(16)[[0, 4, 7], :]\n",
    "emb = model(x_enc)\n",
    "emb.shape\n",
    "# >> (3, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95516da2-743a-4fd0-97d7-6c2d399e57a0",
   "metadata": {},
   "source": [
    "From a usage standpoint, you typically use  `nn.Embedding(num_embeddings=N, embedding_dim=M)`  if you need to encode a categorical entity with N possible \n",
    "unique values with a vector of size M. E.g., you have a vocabulary of size 26 + 10 + 1 = 37 characters (lower-case English alphabet, digits and a space), \n",
    "each should be encoded with a 256-long vector. And an  `nn.Linear`  elsewhere.\n",
    "\n",
    "Both modules implement different subroutines for their intended purposes, e.g.,  \n",
    "- `nn.Embedding`  does not have a  `bias`  parameter, since it does not make sense to add bias to an item value in a look-up table; \n",
    "- `nn.Embedding`  has a  `padding_idx`, which allows one to freeze the representation of some character, but is not implemented for `nn.Linear`.\n",
    "\n",
    "Here is how you could get exactly the same results with both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1df507d4-b909-49d2-8c75-0ae57cfc9c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "via Embedding layer:  tensor([[0.5940, 0.3003, 0.7661, 0.5040],\n",
      "        [0.0210, 0.5245, 0.5118, 0.3714],\n",
      "        [0.4909, 0.2059, 0.3402, 0.2457]], grad_fn=<EmbeddingBackward0>)\n",
      "via linear layer:  tensor([[0.5940, 0.3003, 0.7661, 0.5040],\n",
      "        [0.0210, 0.5245, 0.5118, 0.3714],\n",
      "        [0.4909, 0.2059, 0.3402, 0.2457]], grad_fn=<MmBackward0>)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 16\n",
    "emb_dim = 4\n",
    "\n",
    "linear = nn.Linear(vocab_size, emb_dim, bias=False)\n",
    "embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "\n",
    "w  = torch.rand(vocab_size, emb_dim) #[N,D]\n",
    "linear.weight = nn.parameter.Parameter(w.T) #[D,*]\n",
    "embedding.weight = nn.parameter.Parameter(w) #[N,D]\n",
    "\n",
    "x = torch.tensor([1,2,3])\n",
    "y1 = embedding(x)\n",
    "print(\"via Embedding layer: \", y1)\n",
    "\n",
    "x_enc = torch.eye(16)[[1, 2, 3], :]\n",
    "y2 = linear(x_enc)\n",
    "print(\"via linear layer: \", y2)\n",
    "\n",
    "print(torch.allclose(y1, y2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5adac4-a469-4069-b903-d2c690a31f55",
   "metadata": {},
   "source": [
    "Recall, that  `nn.Embedding` stores the parameters as an `(embedding_dim, num_embeddings)` matrix, \n",
    "but `nn.Linear` stores the parameters as `(in_features, out_features)`. There is no reason for that, other than backward compatibility, see  [PyTorch - shape of nn.Linear weights](https://stackoverflow.com/questions/53465608/pytorch-shape-of-nn-linear-weights)  for explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cfcad7-b3f9-4b51-ab72-d558dd13331f",
   "metadata": {},
   "source": [
    "## Answer 2 TL;DR\n",
    "### TL;DR\n",
    "-   `nn.Embedding`  is for categorical input.\n",
    "-   `nn.Linear`  is for ordinal input.\n",
    "\n",
    "### Explanation\n",
    "\n",
    "You use `nn.Embedding`  when dealing with categorical data, e.g., class labels (0, 1, 2, ...). Because in a lookup table, the value would not be proportional to the key. This behavior suits categorical data, whose value has nothing to do with the semantics.\n",
    "\n",
    "On the other hand,  `nn.Linear`, being a matrix multiplication, does not provide the aforementioned behavior. The input and output are proportional due to the natural of multiplication. Therefore, you use  `nn.Linear`  for ordinal data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564145d5-473a-458d-925d-a15aea4a832d",
   "metadata": {},
   "source": [
    "## Answer 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60461aa7-4f6a-4638-b21b-d9b76d891f8d",
   "metadata": {},
   "source": [
    " [`torch.nn.Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)  is a lookup table; it works the same as  `torch.Tensor`  but with a few twists (like the possibility to use sparse embedding or default value at specified index).\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31eb6a4b-7b70-4514-b351-86aaa5aca374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.weight:  Parameter containing:\n",
      "tensor([[ 0.2495, -0.6835, -0.3323,  0.4207],\n",
      "        [-0.1019,  1.5309,  2.9065, -1.0414],\n",
      "        [ 0.4161, -0.6921, -1.0098,  2.1612]], requires_grad=True)\n",
      "To select the first row of embedding: tensor([[-0.1019,  1.5309,  2.9065, -1.0414]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "embedding = torch.nn.Embedding(3, 4)\n",
    "\n",
    "print(\"embedding.weight: \", embedding.weight)\n",
    "\n",
    "print(\"To select the first row of embedding:\", embedding(torch.tensor([1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73db19d-8290-40a0-a033-a06f75957183",
   "metadata": {},
   "source": [
    "So we took the first row of the embedding. It does nothing more than that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec84f0d-06cb-4b2a-af1d-198a549cab5d",
   "metadata": {},
   "source": [
    "### Where is Embedding Layer used?\n",
    "\n",
    "Usually when we want to encode some meaning (like `word2vec`) for each row (e.g., words being close semantically are close in euclidean space) and possibly train them.\n",
    "\n",
    "### Linear Layer\n",
    "\n",
    "[`torch.nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)  (without bias) is also a  `torch.Tensor`  (weight)  **but**  it does operation on it (and the input) which is essentially:\n",
    "\n",
    "```python\n",
    "output = input.matmul(weight.t())\n",
    "```\n",
    "Every time you call the layer (see  [source code](https://pytorch.org/docs/stable/_modules/torch/nn/functional.html#linear)  and  [functional definition of this layer](https://pytorch.org/docs/stable/nn.functional.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643298b9-1e9f-41a4-9d48-c683c99f030c",
   "metadata": {},
   "source": [
    "### Embedding Layer's Code snippet\n",
    "\n",
    "The Embedding layer in your code snippet does this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "091d0cbb-bc6e-42cf-be4c-dad17bd7d6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProduct(torch.nn.Module):\n",
    "    def __init__(self, n_users, n_movies, n_factors):\n",
    "        # creates two lookup tables \n",
    "        self.user_factors = Embedding(n_users, n_factors)\n",
    "        self.movie_factors = Embedding(n_movies, n_factors)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        users = self.user_factors(x[:,0])\n",
    "        movies = self.movie_factors(x[:,1])\n",
    "        return (users * movies).sum(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01a960a-adc7-481d-bb21-165dce5bfa6b",
   "metadata": {},
   "source": [
    "What it does:\n",
    "-   To creates two lookup tables in  `__init__`\n",
    "-   the layer is called with input of shape  `(batch_size, 2)`:\n",
    "\t-  The first column contains indices of user embeddings\n",
    "    -  The second column contains indices of movie embeddings\n",
    "-   these embeddings are multiplied and summed returning  `(batch_size,)`. So it's different from `nn.Linear` which would return  `(batch_size, out_features)`.\n",
    "\n",
    "This is probably used to train both representations (of users and movies) for some recommender-like system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23889a7-0072-4e11-b847-ae68d5ee4f38",
   "metadata": {},
   "source": [
    "### Other stuff\n",
    "\n",
    "> I know it does some faster computational version of a dot product where one of the matrices is a one-hot encoded matrix and the other is the embedding matrix.\n",
    "\n",
    "No, it doesn't.  `torch.nn.Embedding`  **can be one hot encoded**  and might also be sparse, but \n",
    "depending on the algorithms (and whether those support sparsity) there might be performance boost or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3603da1-51c8-410d-ac8e-cad66a42498b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

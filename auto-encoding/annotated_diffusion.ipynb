{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32c865c1-135d-41f0-a585-b620b587814b",
   "metadata": {},
   "source": [
    "# The Annotated Diffusion Model and Study Notes\n",
    "\n",
    "This is my study note of the blog\n",
    "[The Annotated Diffusion Model](https://huggingface.co/blog/annotated-diffusion) by Niels Rogge and Kashif Rasul.\n",
    "\n",
    "> see link https://huggingface.co/blog/annotated-diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b7c7ce-495d-41cd-8ae3-5190edea4d57",
   "metadata": {},
   "source": [
    "In this blog post, we'll take a deeper look into **Denoising Diffusion Probabilistic Models** (also known as DDPMs, diffusion models, score-based generative models or simply [autoencoders](https://benanne.github.io/2022/01/31/diffusion.html)) as researchers have been able to achieve remarkable results with them for (un)conditional image/audio/video generation. Popular examples (at the time of writing) include [GLIDE](https://arxiv.org/abs/2112.10741) and [DALL-E 2](https://openai.com/dall-e-2/) by OpenAI, [Latent Diffusion](https://github.com/CompVis/latent-diffusion) by the University of Heidelberg and [ImageGen](https://imagen.research.google/) by Google Brain.\n",
    "\n",
    "\n",
    "We'll go over the original DDPM paper by ([Ho et al., 2020](https://arxiv.org/abs/2006.11239)), implementing it step-by-step in PyTorch, based on Phil Wang's [implementation](https://github.com/lucidrains/denoising-diffusion-pytorch) - which itself is based on the [original TensorFlow implementation](https://github.com/hojonathanho/diffusion). Note that the idea of diffusion for generative modeling was actually already introduced in ([Sohl-Dickstein et al., 2015](https://arxiv.org/abs/1503.03585)). However, it took until ([Song et al., 2019](https://arxiv.org/abs/1907.05600)) (at Stanford University), and then ([Ho et al., 2020](https://arxiv.org/abs/2006.11239)) (at Google Brain) who independently improved the approach.\n",
    "\n",
    "Note that there are [several perspectives](https://twitter.com/sedielem/status/1530894256168222722?s=20&t=mfv4afx1GcNQU5fZklpACw) on diffusion models. Here, we employ the discrete-time (latent variable model) perspective, but be sure to check out the other perspectives as well.\n",
    "\n",
    "Alright, let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e46fa56-a273-4095-874d-997aabf02fce",
   "metadata": {},
   "source": [
    "We'll install and import the required libraries first (assuming you have [PyTorch](https://pytorch.org/) installed).\n",
    "\n",
    "```python\n",
    "#!pip install -q -U einops datasets matplotlib tqdm\n",
    "\n",
    "import math\n",
    "from inspect import isfunction\n",
    "from functools import partial\n",
    "\n",
    "#%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange, reduce\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899df568-cfd1-43c5-bdf4-19616969cb0e",
   "metadata": {},
   "source": [
    "## What is a diffusion model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30378c03-527b-49a4-a11d-4d21dacb0da9",
   "metadata": {},
   "source": [
    "A (denoising) diffusion model isn't that complex if you compare it to other generative models such as Normalizing Flows, GANs or VAEs: they all convert noise from some simple distribution to a data sample. This is also the case here where **a neural network learns to gradually denoise data** starting from pure noise. \n",
    "\n",
    "In a bit more detail for images, the set-up consists of 2 processes:\n",
    "* a fixed (or predefined) forward diffusion process $q$ of our choosing, that gradually adds Gaussian noise to an image, until you end up with pure noise\n",
    "* a `learned` reverse denoising diffusion process $p_\\theta$, where a neural network is trained to gradually denoise an image starting from pure noise, until you end up with an actual image.\n",
    "\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/78_annotated-diffusion/diffusion_figure.png\" width=\"600\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "Both the forward and reverse process indexed by $t$ happen for some number of finite time steps $T$ (the DDPM authors use $T=1000$). You start with $t=0$ where you sample a real image $\\mathbf{x}_0$ from your data distribution (let's say an image of a cat from ImageNet), and the forward process samples some noise from a Gaussian distribution at each time step $t$, which is added to the image of the previous time step. Given a sufficiently large $T$ and a well behaved schedule for adding noise at each time step, you end up with what is called an [isotropic Gaussian distribution](https://math.stackexchange.com/questions/1991961/gaussian-distribution-is-isotropic) at $t=T$ via a gradual process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c75a302-6ad9-404d-9c5e-ed6cfb4b2d38",
   "metadata": {},
   "source": [
    "## In more mathematical form\n",
    "\n",
    "Let's write this down more formally, as ultimately we need a tractable loss function which our neural network needs to optimize. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb6de70-7822-4c02-8d1b-0e755e8e7c6f",
   "metadata": {},
   "source": [
    "### Forward diffusion process $q$\n",
    "\n",
    "Let $q(\\mathbf{x}_0)$ be the real data distribution, say of \"real images\". We can sample from this distribution to get an image, $\\mathbf{x}_0 \\sim q(\\mathbf{x}_0)$. We define the forward diffusion process $q(\\mathbf{x}_t | \\mathbf{x}_{t-1})$ which adds Gaussian noise at each time step $t$, according to a known variance schedule $0 < \\beta_1 < \\beta_2 < ... < \\beta_T < 1$ as\n",
    "\n",
    "$$\n",
    "q(\\mathbf{x}_t | \\mathbf{x}_{t-1}) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{1 - \\beta_t} \\mathbf{x}_{t-1}, \\beta_t \\mathbf{I}). \n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "Recall that a normal distribution (also called Gaussian distribution) is defined by 2 parameters: a mean $\\mu$ and a variance $\\sigma^2 \\geq 0$. \n",
    "\n",
    "Basically, each new (slightly noisier) image $\\mathbf{x}_t$ at time step $t$ is drawn from a **conditional Gaussian distribution** with $\\mathbf{\\mu}_t = \\sqrt{1 - \\beta_t} \\mathbf{x}_{t-1}$ and $\\sigma^2_t = \\beta_t$, which we can do by leveraging the <font color='red'>**reparameterization trick**</font> to sample $\\mathbf{\\epsilon_{t-1}} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ and then setting \n",
    "\n",
    "$$\\mathbf{x}_t = \\sqrt{1 - \\beta_t} \\mathbf{x}_{t-1} +  \\sqrt{\\beta_t} \\mathbf{\\epsilon_{t-1}}\n",
    "\\tag{2}\n",
    "$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Note that the $\\beta_t$ aren't constant at each time step $t$ (hence the subscript) --- in fact one defines a so-called <font color='red'>**\"variance schedule\"**</font>, which can be linear, quadratic, cosine, etc. as we will see further (a bit like a learning rate schedule). \n",
    "\n",
    "So starting from $\\mathbf{x}_0$, we end up with $\\mathbf{x}_1,  ..., \\mathbf{x}_t, ..., \\mathbf{x}_T$, where $\\mathbf{x}_T$ is pure Gaussian noise if we set the schedule appropriately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8a692c-b1c5-4a36-9a34-8878efd58206",
   "metadata": {},
   "source": [
    "### Reverse denoising diffusion process $p_\\theta$\n",
    "\n",
    "Now, if we knew the conditional distribution $p(\\mathbf{x}_{t-1} | \\mathbf{x}_t)$, then we could run the process in reverse: by sampling some random Gaussian noise $\\mathbf{x}_T$, and then gradually \"denoise\" it so that we end up with a sample from the real distribution $\\mathbf{x}_0$.\n",
    "\n",
    "However, we don't know $p(\\mathbf{x}_{t-1} | \\mathbf{x}_t)$. It's <font color='red'>**intractable**</font> since it requires knowing the distribution of all possible images in order to calculate this conditional probability. Hence, we're going to leverage a neural network to **approximate (learn) this conditional probability distribution**, let's call it $p_\\theta (\\mathbf{x}_{t-1} | \\mathbf{x}_t)$, with $\\theta$ being the parameters of the neural network, updated by gradient descent. \n",
    "\n",
    "Ok, so we need a neural network to represent a (conditional) probability distribution of the backward process. If we assume this reverse process is Gaussian as well, then recall that any Gaussian distribution is defined by 2 parameters:\n",
    "* a mean parametrized by $\\mu_\\theta$;\n",
    "* a variance parametrized by $\\Sigma_\\theta$;\n",
    "\n",
    "so we can parametrize the process as \n",
    "\n",
    "$$ p_\\theta (\\mathbf{x}_{t-1} | \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\mu_\\theta(\\mathbf{x}_{t},t), \\Sigma_\\theta (\\mathbf{x}_{t},t))\n",
    "\\tag{3}\n",
    "$$\n",
    "\n",
    "where the mean and variance are also conditioned on the noise level $t$.\n",
    "\n",
    "Hence, our neural network needs to learn/represent the mean and variance. However, the DDPM authors decided to **keep the variance fixed, and let the neural network only learn (represent) the mean $\\mu_\\theta$ of this conditional probability distribution**. From the paper:\n",
    "\n",
    "> First, we set $\\Sigma_\\theta ( \\mathbf{x}_t, t) = \\sigma^2_t \\mathbf{I}$ to **untrained** time dependent constants. Experimentally, both $\\sigma^2_t = \\beta_t$ and $\\sigma^2_t  = \\tilde{\\beta}_t$ (see paper) had similar results. \n",
    "\n",
    "This was then later improved in the [Improved diffusion models](https://openreview.net/pdf?id=-NEXDKk8gZ) paper, where a neural network also learns the variance of this backwards process, besides the mean.\n",
    "\n",
    "So we continue, assuming that our neural network only needs to learn/represent the mean of this conditional probability distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624ed131-ea85-4224-a14e-ca6c6833b4cd",
   "metadata": {},
   "source": [
    "#### My study notes\n",
    "> Q1: Why the forward diffusion process is doable or more specifically, fixed and pre-defined (aka non-learning)?\n",
    "\n",
    "It is because there are no unknown parameters in the forward conditional Gaussian probability $q(\\mathbf{x}_t | \\mathbf{x}_{t-1})$ as in Eq (1). We can easily get $\\mathbf{x}_t$ from $\\mathbf{x}_{t-1}$ based on this conditional probability and the **reparameterization trick** as in Eq (2).\n",
    "\n",
    "This reparameterization trick explains why the forward diffusion process is doable and how it is conducted:\n",
    "- At $t=0$, we sample a real image  $x_0$ from a data distribution (e.g., to sample an image of a cat from ImageNet), \n",
    "- Given Eq (2), at new steps $t \\in \\{ 1,2, \\dots, T \\}$ , then we can sample some noise $\\mathbf{\\epsilon_{t-1}}$, which is added to the image $\\mathbf{x_{t-1}}$ of the previous time step. \n",
    "- Then we can get all the new images $\\mathbf{x}_t$ in forward diffusion runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6f13dd-fdda-4f5a-9ad5-9466f5ec4e83",
   "metadata": {},
   "source": [
    "\n",
    "> Q2: Why the reverse denoising diffusion process is intractable?\n",
    "\n",
    "It is because there indeed are unknown parameters in the conditional distribution $p(\\mathbf{x}_{t-1} | \\mathbf{x}_t)$ as in Eq (3). \n",
    "We do not know the mean $\\mu_\\theta$ and the variance $\\Sigma_\\theta$, and hence we <font color='red'>**CANNOT**</font> leverage the reparameterization trick to do the sampling iteratively:\n",
    "\n",
    "$$\\mathbf{x}_{t-1} = \\mu_{\\theta} \\mathbf{x}_{t} +  \\sqrt{\\Sigma_\\theta} \\mathbf{\\epsilon_{t}}\n",
    "\\tag{4}\n",
    "$$\n",
    "\n",
    "We CANNOT get $\\mathbf{x}_{t-1}$ from $\\mathbf{x}_{t}$ based on this unknown conditional probability and the **reparameterization trick** as in Eq (4).\n",
    "\n",
    "\n",
    "Another way to interpret this \"intractability\" is from Bayes' theorem. \n",
    "\n",
    "- If we can reverse the above process and sample from $p(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t)$, we will be able to recreate the true sample from a Gaussian noise input, $\\mathbf{x}_T \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$. Note that if $\\beta_t$ is small enough, $p(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t)$ will also be Gaussian. <font color='red'> Unfortunately, we cannot easily estimate </font>  $p(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t)$ because it needs to use <font color='red'> the entire dataset </font>. \n",
    "- Recall Bayes’ rule:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t) \n",
    "&=  \\frac{ p(\\mathbf{x}_t \\vert \\mathbf{x}_{t-1})  p(\\mathbf{x}_{t-1}) }{ p(\\mathbf{x}_t ) } \\\\\n",
    "& \\xRightarrow[\\text{}]{\\text{w.r.t hiden var. } z}   \\frac{ p(\\mathbf{x}_t \\vert \\mathbf{x}_{t-1})  p(\\mathbf{x}_{t-1}) }{ \\int_{z} p\\left( \\mathbf {x_t}, z \\right) dz } =  \\frac{ p(\\mathbf{x}_t \\vert \\mathbf{x}_{t-1})  p(\\mathbf{x}_{t-1}) }{ \\int_{z} p\\left( \\mathbf {x_t} \\vert z \\right) p\\left( \\mathbf {z} \\right) dz }  \\\\\n",
    "& \\xRightarrow[\\text{}]{\\text{Or w.r.t any prev. } x } \\frac{ p(\\mathbf{x}_t \\vert \\mathbf{x}_{t-1})  p(\\mathbf{x}_{t-1}) }{ \\int_{\\mathbf{x}_{t-1}} p(\\mathbf{x}_t \\vert \\mathbf{x}_{t-1}) p(\\mathbf{x}_{t-1}) d\\mathbf{x}_{t-1} }  \\\\\n",
    "\\end{aligned} \n",
    "\\tag{5}\n",
    "$$\n",
    "\n",
    "- Therefore, the reverse conditional probability $p(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t) $ as shown in Eq (3) and (5), is intractable because **<font color='red'> the integration </font>** is performed over the whole latent $z$ (or equivalently the previous samples $\\mathbf{x}_{t-1}$) space, which is impractical when latent variables are continuous.\n",
    "\n",
    "- It is intractable, but should we do nothing? No! As we know that neural networks are universal function approximators that can approximate any functions to arbitrary precisions. Therefore, we could use a neural network with parameters $\\theta$ to approximate the distribution $p(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t) $, which gives us $p_{\\theta}(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t) $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad968211-6381-4e59-a541-39cae349dd51",
   "metadata": {},
   "source": [
    "## Defining an objective function (by reparametrizing the mean)\n",
    "\n",
    "To derive an objective function to learn the mean of the backward process, the authors observe that the combination of $q$ and $p_\\theta$ can be seen as a variational auto-encoder (VAE) [(Kingma et al., 2013)](https://arxiv.org/abs/1312.6114). Hence, the **variational lower bound** (also called **ELBO (Evidence Lower Bound)**) can be used to minimize the negative log-likelihood with respect to  ground truth data sample $\\mathbf{x}_0$ (we refer to the VAE paper for details regarding ELBO). It turns out that the ELBO for this process is a sum of losses at each time step $t$, \n",
    "\n",
    "$$L = L_0 + L_1 + ... + L_T\n",
    "\\tag{6}\n",
    "$$. \n",
    "\n",
    "By construction of the forward $q$ process and backward process, each term (except for $L_0$) of the loss is actually the **KL divergence between 2 Gaussian distributions** which can be written explicitly as an L2-loss with respect to the means!\n",
    "\n",
    "A direct consequence of the constructed forward process $q$, as shown by Sohl-Dickstein et al., is that we can sample $\\mathbf{x}_t$ at any arbitrary noise level conditioned on $\\mathbf{x}_0$ (<font color='red'> since sums of Gaussians is also Gaussian </font>). This is very convenient:  we don't need to apply $q$ repeatedly in order to sample $\\mathbf{x}_t$.\n",
    "\n",
    "We have that\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{nice property} \\quad q(\\mathbf{x}_t | \\mathbf{x}_0) &= \\cal{N}(\\mathbf{x}_t; \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0, (1- \\bar{\\alpha}_t) \\mathbf{I}) \\\\ \n",
    "\\text{ with } \\alpha_t &:= 1 - \\beta_t \\text{ and } \\\\\n",
    "\\bar{\\alpha}_t &:= \\Pi_{s=1}^{t} \\alpha_s\n",
    "\\end{aligned}\n",
    "\\tag{7}\n",
    "$$.\n",
    "\n",
    "\n",
    "- Let's refer to this equation (7) as the \"nice property\". This means we can sample Gaussian noise and scale it appropriatly and add it to $\\mathbf{x}_0$ to get $\\mathbf{x}_t$ directly. Then similar to Eq (2) leveraging the `reparameterization` trick, we can get \n",
    "\n",
    "$$\\mathbf{x}_t = \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0 + \\sqrt{(1- \\bar{\\alpha}_t)} \\mathbf{\\epsilon}\n",
    "\\tag{8}\n",
    "$$\n",
    "\n",
    "- Note that the $\\bar{\\alpha}_t$ are functions of the known $\\beta_t$ `variance schedule` and thus are also known and can be precomputed. This then allows us, during training, to **optimize random terms of the loss function $L$** (or in other words, to randomly sample $t$ during training and optimize $L_t$).\n",
    "\n",
    "- Another beauty of this property, as shown in Ho et al. is that one can (after some math, for which we refer the reader to [this excellent blog post: What are Diffusion Models?](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)) instead **reparametrize the mean to make the neural network learn (predict) the added noise (via a network $\\mathbf{\\epsilon}_\\theta(\\mathbf{x}_t, t)$) for noise level $t$** in the KL terms which constitute the losses. This means that our neural network becomes a  **<font color='red'> noise predictor </font>**, rather than a (direct) mean predictor. The mean can be computed as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t) &= \\color{cyan}{\\frac{1}{\\sqrt{\\alpha_t}} \\Big( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t) \\Big)} \\\\\n",
    "\\text{Thus } \\mathbf{x}_{t-1} \\sim p(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t) &= \\mathcal{N}(\\mathbf{x}_{t-1}; \\frac{1}{\\sqrt{\\alpha_t}} \\Big( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t) \\Big), \\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t))\n",
    "\\end{aligned}\n",
    "\\tag{9}\n",
    "$$\n",
    "\n",
    "\n",
    "The final objective function $L_t$ then looks as follows (for a random time step $t$ given $\\mathbf{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ ): \n",
    "\n",
    "$$ \\| \\mathbf{\\epsilon} - \\mathbf{\\epsilon}_\\theta(\\mathbf{x}_t, t) \\|^2 = \\| \\mathbf{\\epsilon} - \\mathbf{\\epsilon}_\\theta( \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0 + \\sqrt{(1- \\bar{\\alpha}_t)  } \\mathbf{\\epsilon}, t) \\|^2\n",
    "\\tag{10}\n",
    "$$\n",
    "\n",
    "Here, $\\mathbf{x}_0$ is the initial (real, uncorrupted) image, and we see the direct noise level $t$ sample given by the fixed forward process. $\\mathbf{\\epsilon}$ is the pure noise sampled at time step $t$, and $\\mathbf{\\epsilon}_\\theta (\\mathbf{x}_t, t)$ is our neural network. The neural network is optimized using a simple mean squared error (MSE) between the true and the predicted Gaussian noise.\n",
    "\n",
    "The training algorithm now looks as follows:\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/78_annotated-diffusion/training.png\" width=\"400\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "In other words:\n",
    "* we take a random sample $\\mathbf{x}_0$ from the real unknown and possibily complex data distribution $q(\\mathbf{x}_0)$\n",
    "* we sample a noise level $t$ uniformally between $1$ and $T$ (i.e., a random time step)\n",
    "* we sample some noise from a Gaussian distribution and corrupt the input by this noise at level $t$ (using the nice property defined above)\n",
    "* the neural network is trained to predict this noise based on the corrupted image $\\mathbf{x}_t$ (i.e. noise applied on $\\mathbf{x}_0$ based on known schedule $\\beta_t$)\n",
    "\n",
    "In reality, all of this is done on batches of data, as one uses stochastic gradient descent to optimize neural networks.\n",
    "\n",
    "## The neural network\n",
    "\n",
    "The neural network needs to take in a noised image at a particular time step and return the predicted noise. Note that the predicted noise is a tensor that has the same size/resolution as the input image. So technically, the network takes in and outputs tensors of the same shape. What type of neural network can we use for this? \n",
    "\n",
    "What is typically used here is very similar to that of an [Autoencoder](https://en.wikipedia.org/wiki/Autoencoder), which you may remember from typical \"intro to deep learning\" tutorials. Autoencoders have a so-called \"bottleneck\" layer in between the encoder and decoder. The encoder first encodes an image into a smaller hidden representation called the \"bottleneck\", and the decoder then decodes that hidden representation back into an actual image. This forces the network to only keep the most important information in the bottleneck layer.\n",
    "\n",
    "In terms of architecture, the DDPM authors went for a **U-Net**, introduced by ([Ronneberger et al., 2015](https://arxiv.org/abs/1505.04597)) (which, at the time, achieved state-of-the-art results for medical image segmentation). This network, like any autoencoder, consists of a bottleneck in the middle that makes sure the network learns only the most important information. Importantly, it introduced residual connections between the encoder and decoder, greatly improving gradient flow (inspired by ResNet in [He et al., 2015](https://arxiv.org/abs/1512.03385)).\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/78_annotated-diffusion/unet_architecture.jpg\" width=\"600\" />\n",
    "</div>\n",
    "\n",
    "As can be seen, a U-Net model first downsamples the input (i.e. makes the input smaller in terms of spatial resolution), after which upsampling is performed.\n",
    "\n",
    "Below, we implement this network, step-by-step.\n",
    "\n",
    "### Network helpers\n",
    "\n",
    "First, we define some helper functions and classes which will be used when implementing the neural network. Importantly, we define a `Residual` module, which simply adds the input to the output of a particular function (in other words, adds a residual connection to a particular function).\n",
    "\n",
    "We also define aliases for the up- and downsampling operations.\n",
    "\n",
    "```python\n",
    "def exists(x):\n",
    "    return x is not None\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if isfunction(d) else d\n",
    "\n",
    "\n",
    "def num_to_groups(num, divisor):\n",
    "    groups = num // divisor\n",
    "    remainder = num % divisor\n",
    "    arr = [divisor] * groups\n",
    "    if remainder > 0:\n",
    "        arr.append(remainder)\n",
    "    return arr\n",
    "\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.fn(x, *args, **kwargs) + x\n",
    "\n",
    "\n",
    "def Upsample(dim, dim_out=None):\n",
    "    return nn.Sequential(\n",
    "        nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "        nn.Conv2d(dim, default(dim_out, dim), 3, padding=1),\n",
    "    )\n",
    "\n",
    "\n",
    "def Downsample(dim, dim_out=None):\n",
    "    # No More Strided Convolutions or Pooling\n",
    "    return nn.Sequential(\n",
    "        Rearrange(\"b c (h p1) (w p2) -> b (c p1 p2) h w\", p1=2, p2=2),\n",
    "        nn.Conv2d(dim * 4, default(dim_out, dim), 1),\n",
    "    )\n",
    "```\n",
    "\n",
    "### Position embeddings\n",
    "\n",
    "As the parameters of the neural network are shared across time (noise level), the authors employ sinusoidal position embeddings to encode $t$, inspired by the Transformer ([Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)). This makes the neural network \"know\" at which particular time step (noise level) it is operating, for every image in a batch.\n",
    "\n",
    "The `SinusoidalPositionEmbeddings` module takes a tensor of shape `(batch_size, 1)` as input (i.e. the noise levels of several noisy images in a batch), and turns this into a tensor of shape `(batch_size, dim)`, with `dim` being the dimensionality of the position embeddings. This is then added to each residual block, as we will see further.\n",
    "\n",
    "```python\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n",
    "```\n",
    "\n",
    "### ResNet block\n",
    "\n",
    "Next, we define the core building block of the U-Net model. The DDPM authors employed a Wide ResNet block ([Zagoruyko et al., 2016](https://arxiv.org/abs/1605.07146)), but Phil Wang has replaced the standard convolutional layer by a \"weight standardized\" version, which works better in combination with group normalization (see ([Kolesnikov et al., 2019](https://arxiv.org/abs/1912.11370)) for details).\n",
    "\n",
    "\n",
    "```python\n",
    "class WeightStandardizedConv2d(nn.Conv2d):\n",
    "    \"\"\"\n",
    "    https://arxiv.org/abs/1903.10520\n",
    "    weight standardization purportedly works synergistically with group normalization\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n",
    "\n",
    "        weight = self.weight\n",
    "        mean = reduce(weight, \"o ... -> o 1 1 1\", \"mean\")\n",
    "        var = reduce(weight, \"o ... -> o 1 1 1\", partial(torch.var, unbiased=False))\n",
    "        normalized_weight = (weight - mean) / (var + eps).rsqrt()\n",
    "\n",
    "        return F.conv2d(\n",
    "            x,\n",
    "            normalized_weight,\n",
    "            self.bias,\n",
    "            self.stride,\n",
    "            self.padding,\n",
    "            self.dilation,\n",
    "            self.groups,\n",
    "        )\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, dim_out, groups=8):\n",
    "        super().__init__()\n",
    "        self.proj = WeightStandardizedConv2d(dim, dim_out, 3, padding=1)\n",
    "        self.norm = nn.GroupNorm(groups, dim_out)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x, scale_shift=None):\n",
    "        x = self.proj(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if exists(scale_shift):\n",
    "            scale, shift = scale_shift\n",
    "            x = x * (scale + 1) + shift\n",
    "\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    \"\"\"https://arxiv.org/abs/1512.03385\"\"\"\n",
    "\n",
    "    def __init__(self, dim, dim_out, *, time_emb_dim=None, groups=8):\n",
    "        super().__init__()\n",
    "        self.mlp = (\n",
    "            nn.Sequential(nn.SiLU(), nn.Linear(time_emb_dim, dim_out * 2))\n",
    "            if exists(time_emb_dim)\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        self.block1 = Block(dim, dim_out, groups=groups)\n",
    "        self.block2 = Block(dim_out, dim_out, groups=groups)\n",
    "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x, time_emb=None):\n",
    "        scale_shift = None\n",
    "        if exists(self.mlp) and exists(time_emb):\n",
    "            time_emb = self.mlp(time_emb)\n",
    "            time_emb = rearrange(time_emb, \"b c -> b c 1 1\")\n",
    "            scale_shift = time_emb.chunk(2, dim=1)\n",
    "\n",
    "        h = self.block1(x, scale_shift=scale_shift)\n",
    "        h = self.block2(h)\n",
    "        return h + self.res_conv(x)\n",
    "```\n",
    "\n",
    "### Attention module\n",
    "\n",
    "Next, we define the attention module, which the DDPM authors added in between the convolutional blocks. Attention is the building block of the famous Transformer architecture ([Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)), which has shown great success in various domains of AI, from NLP and vision to [protein folding](https://www.deepmind.com/blog/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology). Phil Wang employs 2 variants of attention: one is regular multi-head self-attention (as used in the Transformer), the other one is a [linear attention variant](https://github.com/lucidrains/linear-attention-transformer) ([Shen et al., 2018](https://arxiv.org/abs/1812.01243)), whose time- and memory requirements scale linear in the sequence length, as opposed to quadratic for regular attention.\n",
    "\n",
    "For an extensive explanation of the attention mechanism, we refer the reader to Jay Allamar's [wonderful blog post](https://jalammar.github.io/illustrated-transformer/).\n",
    "\n",
    "```python\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
    "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
    "        )\n",
    "        q = q * self.scale\n",
    "\n",
    "        sim = einsum(\"b h d i, b h d j -> b h i j\", q, k)\n",
    "        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
    "        attn = sim.softmax(dim=-1)\n",
    "\n",
    "        out = einsum(\"b h i j, b h d j -> b h i d\", attn, v)\n",
    "        out = rearrange(out, \"b h (x y) d -> b (h d) x y\", x=h, y=w)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(nn.Conv2d(hidden_dim, dim, 1), \n",
    "                                    nn.GroupNorm(1, dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
    "        )\n",
    "\n",
    "        q = q.softmax(dim=-2)\n",
    "        k = k.softmax(dim=-1)\n",
    "\n",
    "        q = q * self.scale\n",
    "        context = torch.einsum(\"b h d n, b h e n -> b h d e\", k, v)\n",
    "\n",
    "        out = torch.einsum(\"b h d e, b h d n -> b h e n\", context, q)\n",
    "        out = rearrange(out, \"b h c (x y) -> b (h c) x y\", h=self.heads, x=h, y=w)\n",
    "        return self.to_out(out)\n",
    "```\n",
    "\n",
    "But I like this figure which compares regular attention vs linear attention. This figure is copied from the paper [Efficient Attention: Attention with Linear Complexities](https://arxiv.org/pdf/1812.01243) or the github repo [Linear Attention Transformer](https://github.com/lucidrains/linear-attention-transformer).\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/78_annotated-diffusion/linear-attention.png\" width=\"700\" />\n",
    "</div>\n",
    "\n",
    "See my study notes about [attention and linear attention](attention-and-linear-attention.md).\n",
    "\n",
    "\n",
    "### Group normalization\n",
    "\n",
    "The DDPM authors interleave the convolutional/attention layers of the U-Net with group normalization ([Wu et al., 2018](https://arxiv.org/abs/1803.08494)). Below, we define a `PreNorm` class, which will be used to apply groupnorm before the attention layer, as we'll see further. Note that there's been a [debate](https://tnq177.github.io/data/transformers_without_tears.pdf) about whether to apply normalization before or after attention in Transformers.\n",
    "\n",
    "```python\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.GroupNorm(1, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x)\n",
    "```\n",
    "\n",
    "### Conditional U-Net\n",
    "\n",
    "Now that we've defined all building blocks (position embeddings, ResNet blocks, attention and group normalization), it's time to define the entire neural network. \n",
    "\n",
    "Recall that the job of the network $\\mathbf{\\epsilon}_\\theta(\\mathbf{x}_t, t)$ is to take in a batch of noisy images and their respective noise levels, and output the noise added to the input. More formally:\n",
    "\n",
    "- the network takes a batch of noisy images of shape `(batch_size, num_channels, height, width)` and a batch of noise levels of shape `(batch_size, 1)` as input, and returns a tensor of shape `(batch_size, num_channels, height, width)`\n",
    "\n",
    "The network is built up as follows:\n",
    "* first, a convolutional layer is applied on the batch of noisy images, and position embeddings are computed for the noise levels;\n",
    "* next, a sequence of downsampling stages are applied. Each downsampling stage consists of 2 ResNet blocks + groupnorm + attention + residual connection + a downsample operation;\n",
    "* at the middle of the network, again ResNet blocks are applied, interleaved with attention;\n",
    "* next, a sequence of upsampling stages are applied. Each upsampling stage consists of 2 ResNet  blocks + groupnorm + attention + residual connection + an upsample operation;\n",
    "* finally, a ResNet block followed by a convolutional layer is applied.\n",
    "\n",
    "Ultimately, neural networks stack up layers as if they were lego blocks (but it's important to understand how they work as illustratated in this blog [A Recipe for Training Neural Networks](http://karpathy.github.io/2019/04/25/recipe/)) by Andrej Karpathy.\n",
    "\n",
    "\n",
    "```python\n",
    "class Unet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        init_dim=None,\n",
    "        out_dim=None,\n",
    "        dim_mults=(1, 2, 4, 8),\n",
    "        channels=3,\n",
    "        self_condition=False,\n",
    "        resnet_block_groups=4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # determine dimensions\n",
    "        self.channels = channels\n",
    "        self.self_condition = self_condition\n",
    "        input_channels = channels * (2 if self_condition else 1)\n",
    "\n",
    "        init_dim = default(init_dim, dim)\n",
    "        self.init_conv = nn.Conv2d(input_channels, init_dim, 1, padding=0) # changed to 1 and 0 from 7,3\n",
    "\n",
    "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
    "        in_out = list(zip(dims[:-1], dims[1:]))\n",
    "\n",
    "        block_klass = partial(ResnetBlock, groups=resnet_block_groups)\n",
    "\n",
    "        # time embeddings\n",
    "        time_dim = dim * 4\n",
    "\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(dim),\n",
    "            nn.Linear(dim, time_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(time_dim, time_dim),\n",
    "        )\n",
    "\n",
    "        # layers\n",
    "        self.downs = nn.ModuleList([])\n",
    "        self.ups = nn.ModuleList([])\n",
    "        num_resolutions = len(in_out)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = ind >= (num_resolutions - 1)\n",
    "\n",
    "            self.downs.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
    "                        block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
    "                        Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
    "                        Downsample(dim_in, dim_out)\n",
    "                        if not is_last\n",
    "                        else nn.Conv2d(dim_in, dim_out, 3, padding=1),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        mid_dim = dims[-1]\n",
    "        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
    "        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))\n",
    "        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out)):\n",
    "            is_last = ind == (len(in_out) - 1)\n",
    "\n",
    "            self.ups.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        block_klass(dim_out + dim_in, dim_out, time_emb_dim=time_dim),\n",
    "                        block_klass(dim_out + dim_in, dim_out, time_emb_dim=time_dim),\n",
    "                        Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
    "                        Upsample(dim_out, dim_in)\n",
    "                        if not is_last\n",
    "                        else nn.Conv2d(dim_out, dim_in, 3, padding=1),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.out_dim = default(out_dim, channels)\n",
    "\n",
    "        self.final_res_block = block_klass(dim * 2, dim, time_emb_dim=time_dim)\n",
    "        self.final_conv = nn.Conv2d(dim, self.out_dim, 1)\n",
    "\n",
    "    def forward(self, x, time, x_self_cond=None):\n",
    "        if self.self_condition:\n",
    "            x_self_cond = default(x_self_cond, lambda: torch.zeros_like(x))\n",
    "            x = torch.cat((x_self_cond, x), dim=1)\n",
    "\n",
    "        x = self.init_conv(x)\n",
    "        r = x.clone()\n",
    "\n",
    "        t = self.time_mlp(time)\n",
    "\n",
    "        h = []\n",
    "\n",
    "        for block1, block2, attn, downsample in self.downs:\n",
    "            x = block1(x, t)\n",
    "            h.append(x)\n",
    "\n",
    "            x = block2(x, t)\n",
    "            x = attn(x)\n",
    "            h.append(x)\n",
    "\n",
    "            x = downsample(x)\n",
    "\n",
    "        x = self.mid_block1(x, t)\n",
    "        x = self.mid_attn(x)\n",
    "        x = self.mid_block2(x, t)\n",
    "\n",
    "        for block1, block2, attn, upsample in self.ups:\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = block1(x, t)\n",
    "\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = block2(x, t)\n",
    "            x = attn(x)\n",
    "\n",
    "            x = upsample(x)\n",
    "\n",
    "        x = torch.cat((x, r), dim=1)\n",
    "\n",
    "        x = self.final_res_block(x, t)\n",
    "        return self.final_conv(x)\n",
    "```\n",
    "\n",
    "## Defining the forward diffusion process\n",
    "\n",
    "The forward diffusion process gradually adds noise to an image from the real distribution, in a number of time steps $T$. This happens according to a **variance schedule**. The original DDPM authors employed a linear schedule:\n",
    "\n",
    "> We set the forward process variances to constants\n",
    "increasing linearly from $\\beta_1 = 10^{−4}$\n",
    "to $\\beta_T = 0.02$.\n",
    "\n",
    "However, it was shown in ([Nichol et al., 2021](https://arxiv.org/abs/2102.09672)) that better results can be achieved when employing a cosine schedule. \n",
    "\n",
    "Below, we define various schedules for the $T$ timesteps (we'll choose one later on).\n",
    "\n",
    "```python\n",
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    \"\"\"\n",
    "    cosine schedule as proposed in https://arxiv.org/abs/2102.09672\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0.0001, 0.9999)\n",
    "\n",
    "def linear_beta_schedule(timesteps):\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps)\n",
    "\n",
    "def quadratic_beta_schedule(timesteps):\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    return torch.linspace(beta_start**0.5, beta_end**0.5, timesteps) ** 2\n",
    "\n",
    "def sigmoid_beta_schedule(timesteps):\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    betas = torch.linspace(-6, 6, timesteps)\n",
    "    return torch.sigmoid(betas) * (beta_end - beta_start) + beta_start\n",
    "```\n",
    "\n",
    "To start with, let's use the linear schedule for $T=300$ time steps and define the various variables from the $\\beta_t$ which we will need, such as the cumulative product of the variances $\\bar{\\alpha}_t$. Each of the variables below are just 1-dimensional tensors, storing values from $t$ to $T$. Importantly, we also define an `extract` function, which will allow us to extract the appropriate $t$ index for a batch of indices.\n",
    "\n",
    "```python\n",
    "timesteps = 300\n",
    "\n",
    "# define beta schedule\n",
    "betas = linear_beta_schedule(timesteps=timesteps)\n",
    "\n",
    "# define alphas \n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
    "\n",
    "# calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
    "\n",
    "# calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
    "\n",
    "def extract(a, t, x_shape):\n",
    "    batch_size = t.shape[0]\n",
    "    out = a.gather(-1, t.cpu())\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
    "```\n",
    "\n",
    "We'll illustrate with a cats image how noise is added at each time step of the diffusion process.\n",
    "\n",
    "```python\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw) # PIL image of shape HWC\n",
    "image\n",
    "```\n",
    "<div align=\"center\">\n",
    "<img src=\"images/78_annotated-diffusion/output_cats.jpeg\" width=\"400\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "Noise is added to PyTorch tensors, rather than Pillow Images. We'll first define image transformations that allow us to go from a PIL image to a PyTorch tensor (on which we can add the noise), and vice versa.\n",
    "\n",
    "These transformations are fairly simple: we first normalize images by dividing by $255$ (such that they are in the $[0,1]$ range), and then make sure they are in the $[-1, 1]$ range. From the DPPM paper:\n",
    "\n",
    "> We assume that image data consists of integers in $\\{0, 1, ... , 255\\}$ scaled linearly to $[−1, 1]$. This\n",
    "ensures that the neural network reverse process operates on consistently scaled inputs starting from\n",
    "the standard normal prior $p(\\mathbf{x}_T )$. \n",
    "\n",
    "\n",
    "```python\n",
    "from torchvision.transforms import Compose, ToTensor, Lambda, ToPILImage, CenterCrop, Resize\n",
    "\n",
    "image_size = 128\n",
    "transform = Compose([\n",
    "    Resize(image_size),\n",
    "    CenterCrop(image_size),\n",
    "    ToTensor(), # turn into torch Tensor of shape CHW, divide by 255\n",
    "    Lambda(lambda t: (t * 2) - 1),\n",
    "    \n",
    "])\n",
    "\n",
    "x_start = transform(image).unsqueeze(0)\n",
    "x_start.shape\n",
    "```\n",
    "\n",
    "<div class=\"output stream stdout\">\n",
    "\n",
    "    Output:\n",
    "    ----------------------------------------------------------------------------------------------------\n",
    "    torch.Size([1, 3, 128, 128])\n",
    "\n",
    "</div>\n",
    "\n",
    "We also define the reverse transform, which takes in a PyTorch tensor containing values in $[-1, 1]$ and turn them back into a PIL image:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "reverse_transform = Compose([\n",
    "     Lambda(lambda t: (t + 1) / 2),\n",
    "     Lambda(lambda t: t.permute(1, 2, 0)), # CHW to HWC\n",
    "     Lambda(lambda t: t * 255.),\n",
    "     Lambda(lambda t: t.numpy().astype(np.uint8)),\n",
    "     ToPILImage(),\n",
    "])\n",
    "```\n",
    "\n",
    "Let's verify this:\n",
    "\n",
    "```python\n",
    "reverse_transform(x_start.squeeze())\n",
    "```\n",
    "    \n",
    "<div align=\"center\">\n",
    "<img src=\"images/78_annotated-diffusion/output_cats_verify.png\" width=\"100\" />\n",
    "</div>\n",
    "We can now define the forward diffusion process as in the paper:\n",
    "\n",
    "\n",
    "```python\n",
    "# forward diffusion (using the nice property)\n",
    "def q_sample(x_start, t, noise=None):\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_start)\n",
    "\n",
    "    sqrt_alphas_cumprod_t = extract(sqrt_alphas_cumprod, t, x_start.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x_start.shape\n",
    "    )\n",
    "\n",
    "    return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "```\n",
    "\n",
    "Let's test it on a particular time step:\n",
    "\n",
    "```python\n",
    "def get_noisy_image(x_start, t):\n",
    "  # add noise\n",
    "  x_noisy = q_sample(x_start, t=t)\n",
    "\n",
    "  # turn back into PIL image\n",
    "  noisy_image = reverse_transform(x_noisy.squeeze())\n",
    "\n",
    "  return noisy_image\n",
    "```\n",
    "\n",
    "```python\n",
    "# take time step\n",
    "t = torch.tensor([40])\n",
    "\n",
    "get_noisy_image(x_start, t)\n",
    "```\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"images/78_annotated-diffusion/output_cats_noisy.png\" width=\"100\" />\n",
    "</div>\n",
    "\n",
    "Let's visualize this for various time steps:\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# use seed for reproducability\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# source: https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py\n",
    "def plot(imgs, with_orig=False, row_title=None, **imshow_kwargs):\n",
    "    if not isinstance(imgs[0], list):\n",
    "        # Make a 2d grid even if there's just 1 row\n",
    "        imgs = [imgs]\n",
    "\n",
    "    num_rows = len(imgs)\n",
    "    num_cols = len(imgs[0]) + with_orig\n",
    "    fig, axs = plt.subplots(figsize=(200,200), nrows=num_rows, ncols=num_cols, squeeze=False)\n",
    "    for row_idx, row in enumerate(imgs):\n",
    "        row = [image] + row if with_orig else row\n",
    "        for col_idx, img in enumerate(row):\n",
    "            ax = axs[row_idx, col_idx]\n",
    "            ax.imshow(np.asarray(img), **imshow_kwargs)\n",
    "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "    if with_orig:\n",
    "        axs[0, 0].set(title='Original image')\n",
    "        axs[0, 0].title.set_size(8)\n",
    "    if row_title is not None:\n",
    "        for row_idx in range(num_rows):\n",
    "            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n",
    "\n",
    "    plt.tight_layout()\n",
    "```\n",
    "\n",
    "```python\n",
    "plot([get_noisy_image(x_start, torch.tensor([t])) for t in [0, 50, 100, 150, 199]])\n",
    "```\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"images/78_annotated-diffusion/output_cats_noisy_multiple.png\" width=\"800\" />\n",
    "</div>\n",
    "    \n",
    "This means that we can now define the loss function given the model as follows:\n",
    "\n",
    "```python\n",
    "def p_losses(denoise_model, x_start, t, noise=None, loss_type=\"l1\"):\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_start)\n",
    "\n",
    "    x_noisy = q_sample(x_start=x_start, t=t, noise=noise)\n",
    "    predicted_noise = denoise_model(x_noisy, t)\n",
    "\n",
    "    if loss_type == 'l1':\n",
    "        loss = F.l1_loss(noise, predicted_noise)\n",
    "    elif loss_type == 'l2':\n",
    "        loss = F.mse_loss(noise, predicted_noise)\n",
    "    elif loss_type == \"huber\":\n",
    "        loss = F.smooth_l1_loss(noise, predicted_noise)\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    return loss\n",
    "```\n",
    "\n",
    "The `denoise_model` will be our U-Net defined above. We'll employ the Huber loss between the true and the predicted noise.\n",
    "\n",
    "## Define a PyTorch Dataset + DataLoader\n",
    "\n",
    "Here we define a regular [PyTorch Dataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html). The dataset simply consists of images from a real dataset, like Fashion-MNIST, CIFAR-10 or ImageNet, scaled linearly to $[−1, 1]$.\n",
    "\n",
    "Each image is resized to the same size. Interesting to note is that images are also randomly horizontally flipped. From the paper:\n",
    "\n",
    "> We used random horizontal flips during training for CIFAR10; we tried training both with and without flips, and found flips to improve sample quality slightly.\n",
    "\n",
    "Here we use the 🤗 [Datasets library](https://huggingface.co/docs/datasets/index) to easily load the Fashion MNIST dataset from the [hub](https://huggingface.co/datasets/fashion_mnist). This dataset consists of images which already have the same resolution, namely 28x28.\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "\n",
    "# load dataset from the hub\n",
    "dataset = load_dataset(\"fashion_mnist\")\n",
    "image_size = 28\n",
    "channels = 1\n",
    "batch_size = 128\n",
    "```\n",
    "\n",
    "Next, we define a function which we'll apply on-the-fly on the entire dataset. We use the `with_transform` [functionality](https://huggingface.co/docs/datasets/v2.2.1/en/package_reference/main_classes#datasets.Dataset.with_transform) for that. The function just applies some basic image preprocessing: random horizontal flips, rescaling and finally make them have values in the $[-1,1]$ range.\n",
    "\n",
    "```python\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# define image transformations (e.g. using torchvision)\n",
    "transform = Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda t: (t * 2) - 1)\n",
    "])\n",
    "\n",
    "# define function\n",
    "def transforms(examples):\n",
    "   examples[\"pixel_values\"] = [transform(image.convert(\"L\")) for image in examples[\"image\"]]\n",
    "   del examples[\"image\"]\n",
    "\n",
    "   return examples\n",
    "\n",
    "transformed_dataset = dataset.with_transform(transforms).remove_columns(\"label\")\n",
    "\n",
    "# create dataloader\n",
    "dataloader = DataLoader(transformed_dataset[\"train\"], batch_size=batch_size, shuffle=True)\n",
    "```\n",
    "\n",
    "```python\n",
    "batch = next(iter(dataloader))\n",
    "print(batch.keys())\n",
    "```\n",
    "\n",
    "<div class=\"output stream stdout\">\n",
    "\n",
    "    Output:\n",
    "    ----------------------------------------------------------------------------------------------------\n",
    "    dict_keys(['pixel_values'])\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "## Sampling\n",
    "\n",
    "As we'll sample from the model during training (in order to track progress), we define the code for that below. Sampling is summarized in the paper as Algorithm 2:\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"images/78_annotated-diffusion/sampling.png\" width=\"500\" />\n",
    "</div>\n",
    "\n",
    "Generating new images from a diffusion model happens by reversing the diffusion process: we start from $T$, where we sample pure noise from a Gaussian distribution, and then use our neural network to gradually denoise it (using the conditional probability it has learned), until we end up at time step $t = 0$. As shown above, we can derive a slighly less denoised image $\\mathbf{x}_{t-1 }$ by plugging in the reparametrization of the mean, using our noise predictor. Remember that the variance is known ahead of time.\n",
    "\n",
    "Ideally, we end up with an image that looks like it came from the real data distribution.\n",
    "\n",
    "The code below implements this.\n",
    "\n",
    "```python\n",
    "@torch.no_grad()\n",
    "def p_sample(model, x, t, t_index):\n",
    "    betas_t = extract(betas, t, x.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x.shape\n",
    "    )\n",
    "    sqrt_recip_alphas_t = extract(sqrt_recip_alphas, t, x.shape)\n",
    "    \n",
    "    # Equation 11 in the paper\n",
    "    # Use our model (noise predictor) to predict the mean\n",
    "    model_mean = sqrt_recip_alphas_t * (\n",
    "        x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t\n",
    "    )\n",
    "\n",
    "    if t_index == 0:\n",
    "        return model_mean\n",
    "    else:\n",
    "        posterior_variance_t = extract(posterior_variance, t, x.shape)\n",
    "        noise = torch.randn_like(x)\n",
    "        # Algorithm 2 line 4:\n",
    "        return model_mean + torch.sqrt(posterior_variance_t) * noise \n",
    "\n",
    "# Algorithm 2 (including returning all images)\n",
    "@torch.no_grad()\n",
    "def p_sample_loop(model, shape):\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    b = shape[0]\n",
    "    # start from pure noise (for each example in the batch)\n",
    "    img = torch.randn(shape, device=device)\n",
    "    imgs = []\n",
    "\n",
    "    for i in tqdm(reversed(range(0, timesteps)), desc='sampling loop time step', total=timesteps):\n",
    "        img = p_sample(model, img, torch.full((b,), i, device=device, dtype=torch.long), i)\n",
    "        imgs.append(img.cpu().numpy())\n",
    "    return imgs\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(model, image_size, batch_size=16, channels=3):\n",
    "    return p_sample_loop(model, shape=(batch_size, channels, image_size, image_size))\n",
    "```\n",
    "\n",
    "Note that the code above is a simplified version of the original implementation. We found our simplification (which is in line with Algorithm 2 in the paper) to work just as well as the [original, more complex implementation](https://github.com/hojonathanho/diffusion/blob/master/diffusion_tf/diffusion_utils.py), which employs [clipping](https://github.com/hojonathanho/diffusion/issues/5).\n",
    "\n",
    "## Train the model\n",
    "\n",
    "Next, we train the model in regular PyTorch fashion. We also define some logic to periodically save generated images, using the `sample` method defined above.\n",
    "\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "\n",
    "def num_to_groups(num, divisor):\n",
    "    groups = num // divisor\n",
    "    remainder = num % divisor\n",
    "    arr = [divisor] * groups\n",
    "    if remainder > 0:\n",
    "        arr.append(remainder)\n",
    "    return arr\n",
    "\n",
    "results_folder = Path(\"./results\")\n",
    "results_folder.mkdir(exist_ok = True)\n",
    "save_and_sample_every = 1000\n",
    "```\n",
    "\n",
    "Below, we define the model, and move it to the GPU. We also define a standard optimizer (Adam).\n",
    "\n",
    "```python\n",
    "from torch.optim import Adam\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = Unet(\n",
    "    dim=image_size,\n",
    "    channels=channels,\n",
    "    dim_mults=(1, 2, 4,)\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "```\n",
    "\n",
    "Let's start training!\n",
    "\n",
    "```python\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "epochs = 6\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for step, batch in enumerate(dataloader):\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      batch_size = batch[\"pixel_values\"].shape[0]\n",
    "      batch = batch[\"pixel_values\"].to(device)\n",
    "\n",
    "      # Algorithm 1 line 3: sample t uniformally for every example in the batch\n",
    "      t = torch.randint(0, timesteps, (batch_size,), device=device).long()\n",
    "\n",
    "      loss = p_losses(model, batch, t, loss_type=\"huber\")\n",
    "\n",
    "      if step % 100 == 0:\n",
    "        print(\"Loss:\", loss.item())\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # save generated images\n",
    "      if step != 0 and step % save_and_sample_every == 0:\n",
    "        milestone = step // save_and_sample_every\n",
    "        batches = num_to_groups(4, batch_size)\n",
    "        all_images_list = list(map(lambda n: sample(model, batch_size=n, channels=channels), batches))\n",
    "        all_images = torch.cat(all_images_list, dim=0)\n",
    "        all_images = (all_images + 1) * 0.5\n",
    "        save_image(all_images, str(results_folder / f'sample-{milestone}.png'), nrow = 6)\n",
    "```\n",
    "\n",
    "<div class=\"output stream stdout\">\n",
    "\n",
    "    Output:\n",
    "    ----------------------------------------------------------------------------------------------------\n",
    "    Loss: 0.46477368474006653\n",
    "    Loss: 0.12143351882696152\n",
    "    Loss: 0.08106148988008499\n",
    "    Loss: 0.0801810547709465\n",
    "    Loss: 0.06122320517897606\n",
    "    Loss: 0.06310459971427917\n",
    "    Loss: 0.05681884288787842\n",
    "    Loss: 0.05729678273200989\n",
    "    Loss: 0.05497899278998375\n",
    "    Loss: 0.04439849033951759\n",
    "    Loss: 0.05415581166744232\n",
    "    Loss: 0.06020551547408104\n",
    "    Loss: 0.046830907464027405\n",
    "    Loss: 0.051029372960329056\n",
    "    Loss: 0.0478244312107563\n",
    "    Loss: 0.046767622232437134\n",
    "    Loss: 0.04305662214756012\n",
    "    Loss: 0.05216279625892639\n",
    "    Loss: 0.04748568311333656\n",
    "    Loss: 0.05107741802930832\n",
    "    Loss: 0.04588869959115982\n",
    "    Loss: 0.043014321476221085\n",
    "    Loss: 0.046371955424547195\n",
    "    Loss: 0.04952816292643547\n",
    "    Loss: 0.04472338408231735\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "## Sampling (inference)\n",
    "\n",
    "To sample from the model, we can just use our sample function defined above:\n",
    "\n",
    "\n",
    "```python\n",
    "# sample 64 images\n",
    "samples = sample(model, image_size=image_size, batch_size=64, channels=channels)\n",
    "\n",
    "# show a random one\n",
    "random_index = 5\n",
    "plt.imshow(samples[-1][random_index].reshape(image_size, image_size, channels), cmap=\"gray\")\n",
    "```\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"images/78_annotated-diffusion/output.png\" width=\"300\" >\n",
    "</div>\n",
    "\n",
    "Seems like the model is capable of generating a nice T-shirt! Keep in mind that the dataset we trained on is pretty low-resolution (28x28).\n",
    "\n",
    "We can also create a gif of the denoising process:\n",
    "\n",
    "```python\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "random_index = 53\n",
    "\n",
    "fig = plt.figure()\n",
    "ims = []\n",
    "for i in range(timesteps):\n",
    "    im = plt.imshow(samples[i][random_index].reshape(image_size, image_size, channels), cmap=\"gray\", animated=True)\n",
    "    ims.append([im])\n",
    "\n",
    "animate = animation.ArtistAnimation(fig, ims, interval=50, blit=True, repeat_delay=1000)\n",
    "animate.save('diffusion.gif')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"images/78_annotated-diffusion/diffusion-sweater.gif\" width=\"300\" />\n",
    "</div>\n",
    "\n",
    "## Follow-up reads\n",
    "\n",
    "Note that the DDPM paper showed that diffusion models are a promising direction for (un)conditional image generation. This has since then (immensely) been improved, most notably for text-conditional image generation. Below, we list some important (but far from exhaustive) follow-up works:\n",
    "\n",
    "- Improved Denoising Diffusion Probabilistic Models ([Nichol et al., 2021](https://arxiv.org/abs/2102.09672)): finds that learning the variance of the conditional distribution (besides the mean) helps in improving performance\n",
    "- Cascaded Diffusion Models for High Fidelity Image Generation ([Ho et al., 2021](https://arxiv.org/abs/2106.15282)): introduces cascaded diffusion, which comprises a pipeline of multiple diffusion models that generate images of increasing resolution for high-fidelity image synthesis\n",
    "- Diffusion Models Beat GANs on Image Synthesis ([Dhariwal et al., 2021](https://arxiv.org/abs/2105.05233)): show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models by improving the U-Net architecture, as well as introducing classifier guidance\n",
    "- Classifier-Free Diffusion Guidance ([Ho et al., 2021](https://openreview.net/pdf?id=qw8AKxfYbI)): shows that you don't need a classifier for guiding a diffusion model by jointly training a conditional and an unconditional diffusion model with a single neural network\n",
    "- Hierarchical Text-Conditional Image Generation with CLIP Latents (DALL-E 2) ([Ramesh et al., 2022](https://cdn.openai.com/papers/dall-e-2.pdf)): uses a prior to turn a text caption into a CLIP image embedding, after which a diffusion model decodes it into an image\n",
    "- Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding (ImageGen) ([Saharia et al., 2022](https://arxiv.org/abs/2205.11487)): shows that combining a large pre-trained language model (e.g. T5) with cascaded diffusion works well for text-to-image synthesis\n",
    "\n",
    "Note that this list only includes important works until the time of writing, which is June 7th, 2022.\n",
    "\n",
    "For now, it seems that the main (perhaps only) disadvantage of diffusion models is that they require multiple forward passes to generate an image (which is not the case for generative models like GANs). However, there's [research going on](https://arxiv.org/abs/2204.13902) that enables high-fidelity generation in as few as 10 denoising steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5869cd2-5ba2-430b-9640-ed63a6caedec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
